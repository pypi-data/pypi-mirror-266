{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup required for all notebooks\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "**This step of our solution design covers running setup steps that need to be run prior to any other notebook being run.**\n",
    "\n",
    "1. Prerequisite: a `Python 3.11` conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install packages listed in the `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import importlib.util\n",
    "import fmbench.scripts\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from typing import Dict, List, Optional\n",
    "from sagemaker import get_execution_role\n",
    "import importlib.resources as pkg_resources\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config.yml file\n",
    "------\n",
    "\n",
    "The config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations, and model configurations like the version of the model, the endpoint name, model_id that needs to be deployed. Configurations also support the gives instance type to be used, for example: \"ml.g5.24xlarge\", the image uri, whether or not to deploy this given model, followed by an inference script \"jumpstart.py\" which supports the inference script for jumpstart models to deploy the model in this deploy notebook. \n",
    "\n",
    "View the contents of the config yml file below and how it is loaded and used throughout this notebook with deploying the model endpoints asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the config.yml file referring to the globals.py file\n",
    "config = load_config(CONFIG_FILE)\n",
    "\n",
    "## configure the aws region and execution role\n",
    "aws_region = config['aws']['region']\n",
    "\n",
    "\n",
    "try:\n",
    "    sagemaker_execution_role = get_execution_role()\n",
    "    config['aws']['sagemaker_execution_role'] = sagemaker_execution_role\n",
    "    logger.info(f\"determined SageMaker exeuction role from get_execution_role\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"could not determine SageMaker execution role, error={e}\")\n",
    "    logger.info(f\"going to look for execution role in config file..\")\n",
    "    sagemaker_execution_role = config['aws'].get('sagemaker_execution_role')\n",
    "    if sagemaker_execution_role is not None:\n",
    "        logger.info(f\"found SageMaker execution role in config file..\")\n",
    "\n",
    "logger.info(f\"aws_region={aws_region}, sagemaker_execution_role={sagemaker_execution_role}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download any scripts from Amazon S3\n",
    "\n",
    "Users can upload scripts to S3 which contain custom code for deployment and inference. We download these scripts here and place them in the `fmbench` package installation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3', region_name=aws_region)\n",
    "\n",
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "# Ensure the scripts directory exists\n",
    "scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "read_bucket = config['s3_read_data']['read_bucket']\n",
    "logger.info(f\"the read bucket is --> {read_bucket} for reading the script files\")\n",
    "scripts_prefix = config['s3_read_data']['scripts_prefix']\n",
    "logger.info(f\"the scripts directory is --> {scripts_prefix} for reading the script file names\")\n",
    "script_files = config['s3_read_data'].get('script_files', [])\n",
    "logger.info(f\"Extracted script files that the user has provided --> {script_files}\")\n",
    "\n",
    "# Download script files to the fmbench.scripts directory\n",
    "try:\n",
    "    for script_name in script_files:\n",
    "        # do os.path.join\n",
    "        s3_script_path = f\"{scripts_prefix}/{script_name}\"\n",
    "        ## take this out of the loop \n",
    "        logger.info(f\"the script path for where the scripts you have entered in s3 will be installed --> {s3_script_path}\")\n",
    "        local_script_path = scripts_dir / script_name\n",
    "        logger.info(f\"Downloading {s3_script_path} to {local_script_path}\")\n",
    "        s3_client.download_file(read_bucket, s3_script_path, str(local_script_path))\n",
    "except ClientError as error:\n",
    "    logger.error(f\"Failed to download script files: {error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
