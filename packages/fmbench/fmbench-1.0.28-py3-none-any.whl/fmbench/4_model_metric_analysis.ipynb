{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Analysis for LLaMa-2 benchmarking\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes the chunk of taking the metrics generated and creating visualizations from it for further analysis to make decisions more quicker, efficient, and cost optimal.\n",
    "\n",
    "- In this file, we will go over and create side by side visualizations of different models deployed, how their inference latency is impacted based on the concurrency level, instance size and different model configurations. Using these visualizations and charts, making executive decisions, saving on time and cost becomes critical. \n",
    "\n",
    "\n",
    "- In this notebook, we will also record the error rates for each of the deployed model endpoints based on how it ran against different metrics as specified above. These visualizations will be applicable and work for any and every jumpstart and non jumpstart model if deployed correctly using the prior steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tomark import Tomark\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil.parser import parse\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# rcParams for configuring Matplotlib settings\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:    \n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}\")\n",
    "    df_per_inference.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column in the dataframe for clarity of the instance parameter of the model used\n",
    "df_per_inference = df_per_inference.rename(columns={\"instance_type\": \"instance\"})\n",
    "\n",
    "if df_per_inference.latency.min() < 1:\n",
    "    print(\"multiplying by 1000\")\n",
    "    latency_units = \"milliseconds\"\n",
    "    multiplier = 1000\n",
    "    step_size = 500\n",
    "    df_per_inference.latency = df_per_inference.latency*1000\n",
    "else:\n",
    "    multiplier = 10\n",
    "    step_size = 5\n",
    "    latency_units = \"seconds\"\n",
    "\n",
    "## Initializing yticks and title for the chart\n",
    "yticks: Optional[List] = None\n",
    "title: Optional[str] = None\n",
    "\n",
    "if config['report'].get('latency_vs_token_len_chart'):\n",
    "    yticks: List = config['report']['latency_vs_token_len_chart'].get('y_ticks')\n",
    "    title: str = config['report']['latency_vs_token_len_chart'].get('title')\n",
    "\n",
    "if title is None:\n",
    "    title = \"Effect of token length on inference latency\"\n",
    "\n",
    "if len(df_per_inference.instance.unique()) == 1:\n",
    "    # This created a FacetGrid for plotting multiple scatter plots based on 'instance' and 'concurrency' categories\n",
    "    g = sns.FacetGrid(df_per_inference, col=\"concurrency\", hue=\"instance\", height=3.5, aspect=1.25, col_wrap=3)\n",
    "else: \n",
    "    g = sns.FacetGrid(df_per_inference, col=\"concurrency\", row=\"instance\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "\n",
    "## Subtitle of the facetgrid\n",
    "g.fig.suptitle(title)\n",
    "\n",
    "# # This will map a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"prompt_tokens\", \"latency\")\n",
    "\n",
    "# Set the y-axis label for all plots\n",
    "g = g.set_ylabels(f\"Latency ({latency_units})\")\n",
    "    \n",
    "if yticks is None:\n",
    "    # Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "    yticks: List = list(range(0, (int(df_per_inference.latency.max())//multiplier+2)*multiplier, step_size))\n",
    "\n",
    "g = g.set(yticks=yticks)\n",
    "\n",
    "# Set the x-axis label for all plots as the prompt length or tokens\n",
    "g = g.set_xlabels(\"Prompt length (tokens)\")\n",
    "\n",
    "# Create a bytes buffer to save the plot\n",
    "buffer = io.BytesIO()\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)  # Rewind buffer to the beginning\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, TOKENS_VS_LATENCY_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{TOKENS_VS_LATENCY_PLOT_FNAME}\")\n",
    "\n",
    "# Optionally, display the plot\n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.latency.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the all metrics file path and read it to generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(BUCKET_NAME, all_metrics_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_all_metrics = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{all_metrics_fpath} read into dataframe of shape {df_all_metrics.shape}\")\n",
    "    df_all_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "df_all_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying all of the available columns in the all metrics dataframe\n",
    "df_all_metrics.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the number of experiment names within the metrics dataframe, instance types and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = df_all_metrics.experiment_name.unique()\n",
    "instance_types = df_all_metrics.instance_type.unique()\n",
    "model_names = df_all_metrics.ModelName.unique()\n",
    "logger.info(f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types, {len(model_names)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract some of the columns\n",
    "relevant_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                       'concurrency',\n",
    "                         'error_rate',\n",
    "                           'prompt_token_count_mean',\n",
    "                             'prompt_token_throughput',\n",
    "                               'completion_token_count_mean',\n",
    "                                 'completion_token_throughput',\n",
    "                                   'latency_mean',\n",
    "                                      'latency_p50',\n",
    "                                        'latency_p95',\n",
    "                                         'latency_p99',\n",
    "                                          'transactions_per_minute']\n",
    "\n",
    "## initialize a group by columns to use further in generating portions of the dataframe and filtering it\n",
    "group_by_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                      'concurrency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'experiment_counts.csv' to store metrics on experiment name, the payload file, concurrency and the total counts associated to that given experiment to visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_all_metrics[group_by_cols].value_counts().reset_index()\n",
    "\n",
    "# Convert df_counts to CSV format\n",
    "csv_buffer = io.StringIO()\n",
    "df_counts.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "COUNTS_FNAME = \"experiment_counts.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, COUNTS_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(csv_data, BUCKET_NAME, \"\", METRICS_DIR, COUNTS_FNAME)\n",
    "logger.info(f\"Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the mean error rates for each experiment with different congifurations using the same columns of interest used in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates = df_all_metrics.groupby(group_by_cols).agg({'error_rate': 'mean'}).reset_index()\n",
    "df_error_rates = df_error_rates.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_error_rates.to_csv(csv_buffer, index=False)\n",
    "error_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "ERROR_RATES_FNAME: str = \"error_rates.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, ERROR_RATES_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(error_csv, BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_FNAME)\n",
    "logger.info(f\"Error Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Inference error rates across different concurrency levels and instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates = df_error_rates.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "\n",
    "# Clean up the dataset names by removing json file extensions and prefixes\n",
    "df_error_rates.dataset = df_error_rates.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "\n",
    "# this creates a facetGrid for plotting scatter plots based on 'instance' and 'dataset'\n",
    "g = sns.FacetGrid(df_error_rates, col=\"instance\", row=\"dataset\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "\n",
    "# Maps a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"concurrency\", \"error_rate\")\n",
    "\n",
    "# Create a subtitle\n",
    "g.fig.suptitle(\"Inference error rates for different concurrency levels and instance types\")\n",
    "\n",
    "## Set x and y labels for this chart\n",
    "g = g.set_ylabels(\"Error rate (failed / total inferences)\")\n",
    "g = g.set_xlabels(\"Concurrency level\")\n",
    "\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{ERROR_RATES_PLOT_FNAME}\")\n",
    "\n",
    "## Display the plot \n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for the df elements that have error rates above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "df_error_rates_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a dataframe to get the mean of the columns in consideration\n",
    "df_summary_metrics = df_all_metrics[relevant_cols].groupby(group_by_cols).mean().reset_index()\n",
    "\n",
    "# ugly way of doing this, will refactor this later (maybe)\n",
    "df_summary_metrics.fillna(PLACE_HOLDER, inplace=True)\n",
    "int_cols = ['prompt_token_count_mean', 'prompt_token_throughput', 'completion_token_count_mean', 'completion_token_throughput', 'transactions_per_minute']\n",
    "for ic in int_cols:\n",
    "    df_summary_metrics[ic] = df_summary_metrics[ic].astype(int)\n",
    "\n",
    "df_summary_metrics.replace(PLACE_HOLDER, np.nan, inplace=True)\n",
    "df_summary_metrics.latency_mean\t= df_summary_metrics.latency_mean.round(2)\n",
    "df_summary_metrics.latency_p50 = df_summary_metrics.latency_p50.round(2)\n",
    "df_summary_metrics.latency_p95 = df_summary_metrics.latency_p95.round(2)\n",
    "df_summary_metrics.latency_p99 = df_summary_metrics.latency_p99.round(2)\n",
    "df_summary_metrics.error_rate\t= df_summary_metrics.error_rate.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics.to_csv(csv_buffer, index=False)\n",
    "summary_metrics_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name for S3 based on the original file path\n",
    "summary_file_name = all_metrics_fpath.replace(\"all_metrics\", \"all_metrics_summary\").split('/')[-1] \n",
    "summary_s3_path = os.path.join(METRICS_DIR, summary_file_name)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_csv, BUCKET_NAME, \"\", METRICS_DIR, summary_file_name)\n",
    "logger.info(f\"Summary metrics DataFrame saved to s3://{BUCKET_NAME}/{summary_s3_path}\")\n",
    "\n",
    "df_summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_nz = df_summary_metrics[df_summary_metrics.error_rate == 0]\n",
    "logger.info(f\"there are {len(df_summary_metrics_nz)} entries out of {len(df_summary_metrics)} in the summary data for which error rate is 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics[df_summary_metrics.payload_file.str.contains(config['metrics']['dataset_of_interest'])]\n",
    "logger.info(f\"shape of dataframe with summary metrics for {config['metrics']['dataset_of_interest']} is {df_summary_metrics_dataset.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(csv_buffer, index=False)\n",
    "metrics_dataset = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_dataset, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_W_PRICING_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{SUMMARY_METRICS_W_PRICING_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_metrics_for_dataset = df_all_metrics.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "df_all_metrics_for_dataset.dataset = df_all_metrics_for_dataset.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "ds = config['metrics']['dataset_of_interest']\n",
    "df_all_metrics_for_dataset = df_all_metrics_for_dataset[df_all_metrics_for_dataset.dataset.str.contains(ds)]\n",
    "# df_all_metrics_for_dataset.concurrency = df_all_metrics_for_dataset.concurrency.astype(str)\n",
    "row_order = list(df_all_metrics_for_dataset[[\"instance\", \"latency_mean\"]].groupby(\"instance\").mean(\"latency_mean\").reset_index()[\"instance\"])\n",
    "print(row_order)\n",
    "sns_plot = sns.catplot(\n",
    "    data=df_all_metrics_for_dataset, x='concurrency', y='latency_mean',\n",
    "    col='instance', kind='box', col_wrap=len(row_order), hue=\"instance\", row_order=row_order #, height=4.5, aspect=1.25\n",
    ")\n",
    "sns_plot._legend.remove()\n",
    "sns_plot.fig.suptitle(f\"Effect of concurrency on inference latency for each instance type for the {ds} dataset\\n\\n\")\n",
    "sns_plot = sns_plot.set_ylabels(\"Latency (seconds)\")\n",
    "sns_plot = sns_plot.set_xlabels(\"Concurrency level\")\n",
    "sns_plot.fig.subplots_adjust(top=0.8)\n",
    "\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pricing = pd.DataFrame.from_dict(config['pricing'], orient='index').reset_index()\n",
    "df_pricing.columns = ['instance_type', 'price_per_hour']\n",
    "# fpath: str = os.path.join(METRICS_DIR, INSTANCE_PRICING_PER_HOUR_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_pricing.to_csv(csv_buffer, index=False)\n",
    "df_pricing_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_pricing_data, BUCKET_NAME, \"\", METRICS_DIR, INSTANCE_PRICING_PER_HOUR_FNAME)\n",
    "\n",
    "df_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = pd.merge(df_summary_metrics_dataset, df_pricing, how='left')\n",
    "logger.info(f\"df_summary_metrics_dataset ----> {df_summary_metrics_dataset}\")\n",
    "logger.info(f\"df_summary_metrics_dataset ----> {df_summary_metrics_dataset}\")\n",
    "df_summary_metrics_dataset['price_per_txn'] = (df_summary_metrics_dataset['price_per_hour']/60)/df_summary_metrics_dataset['transactions_per_minute']\n",
    "df_summary_metrics_dataset['price_per_token'] = (df_summary_metrics_dataset['price_per_txn']/(df_summary_metrics_dataset['prompt_token_count_mean'] \n",
    "                                                                                                      + df_summary_metrics_dataset['completion_token_count_mean']))\n",
    "df_summary_metrics_dataset['price_per_token'] = df_summary_metrics_dataset['price_per_token'].apply(lambda x: f\"{x:.8f}\")\n",
    "price_per_tx_wt = config['metrics']['weights']['price_per_tx_wt']\n",
    "latency_wt = config['metrics']['weights']['latenct_wt']\n",
    "#df_summary_metrics_dataset['score'] = price_per_tx_wt*(1/df_summary_metrics_dataset['price_per_txn']) + latency_wt*(1/df_summary_metrics_dataset['latency_mean'])\n",
    "df_summary_metrics_dataset['score'] = 0.5*(1/df_summary_metrics_dataset['price_per_txn']) + 0.5*(1/df_summary_metrics_dataset['latency_mean'])\n",
    "\n",
    "\"\"\"\n",
    "df_summary_metrics_dataset['rank'] = (df_summary_metrics_dataset.sort_values(by=\"score\", ascending=False)\n",
    "                      .groupby(['instance_type'])['concurrency']\n",
    "                      .rank(method='first', ascending=False)\n",
    "                   )\n",
    "\"\"\"\n",
    "df_summary_metrics_dataset = df_summary_metrics_dataset.sort_values(by=\"score\", ascending=False)\n",
    "file_path_df = os.path.join(METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(file_path_df, index=False)\n",
    "summary_metrics_dataset_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_dataset_csv, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best option overall and for each instance type\n",
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset[df_summary_metrics_dataset.score == df_summary_metrics_dataset.score.max()]\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset_overall.to_csv(csv_buffer, index=False)\n",
    "metrics_overall_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_overall_data, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME)\n",
    "\n",
    "df_summary_metrics_dataset_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset_overall.round(6)\n",
    "df_summary_metrics_dataset_overall.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics_dataset.dropna()\n",
    "idx = df_summary_metrics_dataset.groupby(['instance_type']).score.idxmax()\n",
    "logger.info(f\"shape of df_summary_metrics_dataset={df_summary_metrics_dataset.shape}, idx={idx}\")\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_dataset.loc[idx]\n",
    "logger.info(f\"shape of df_summary_metrics_best_option_instance_type={df_summary_metrics_best_option_instance_type.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_best_option_instance_type.to_csv(csv_buffer, index=False)\n",
    "best_option = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(best_option, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME)\n",
    "\n",
    "df_summary_metrics_best_option_instance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_best_option_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_per_tx = df_summary_metrics_best_option_instance_type.price_per_txn.min()\n",
    "txn_count_for_showing_cost: int = config[\"report\"][\"txn_count_for_showing_cost\"]\n",
    "\n",
    "price_tx_col_name = f\"price_per_tx_{txn_count_for_showing_cost}_txn\"\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = df_summary_metrics_best_option_instance_type.price_per_txn * txn_count_for_showing_cost\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = round(df_summary_metrics_best_option_instance_type[price_tx_col_name], 2)\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_best_option_instance_type.sort_values(by=price_tx_col_name)\n",
    "sns_plot = sns.barplot(df_summary_metrics_best_option_instance_type, x=\"instance_type\", y=price_tx_col_name, hue=\"instance_type\")\n",
    "title: str = f\"Comparing performance of {config['general']['model_name']} across instance types for {config['metrics']['dataset_of_interest']} dataset\"\n",
    "sns_plot.set(xlabel=\"Instance type\", ylabel=f\"Cost per {txn_count_for_showing_cost:,} transactions (USD)\", title=title)\n",
    "num_instance_types = len(df_summary_metrics_dataset.instance_type.unique())\n",
    "for r in df_summary_metrics_best_option_instance_type.iterrows():\n",
    "    x = r[1]['instance_type']\n",
    "    if num_instance_types == 1:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_single_instance\"]\n",
    "    else:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_gt_one_instance\"]\n",
    "    \n",
    "    print(f\"v_shift={v_shift}\")    \n",
    "    y = r[1][price_tx_col_name] + v_shift\n",
    "    text = f\"{r[1]['transactions_per_minute']} txn/min, {r[1]['latency_mean']}s per txn\"\n",
    "    print(f\"x={x}, y={y}, text={text}\")\n",
    "    sns_plot.text(x, y, text, \n",
    "       fontsize = 8,          # Size\n",
    "       #fontstyle = \"oblique\",  # Style\n",
    "       color = \"red\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "\n",
    "business_summary_plot_fpath: str = os.path.join(METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME)\n",
    "sns_plot.figure.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", \"\", business_summary_plot_fpath)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\")\n",
    "\n",
    "## Display the plot \n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_summary_metrics_best_option_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_csv_content_fpath = os.path.join(METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"the cost information can be found in the csv file here -> {cost_csv_content_fpath}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, cost_csv_content_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_cost_metrics = pd.read_csv(io.StringIO(cost_content))\n",
    "    logger.info(f\"{cost_csv_content_fpath} read into dataframe of shape {df_cost_metrics.shape}\")\n",
    "    df_cost_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "# df_cost_metrics.fillna('', inplace=True)\n",
    "\n",
    "df_cost_metrics.head()\n",
    "\n",
    "# Convert df_cost_metrics to Markdown table\n",
    "cost_mkdn_table = Tomark.table(df_cost_metrics.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUSINESS_SUMMARY: str = \"\"\"We did performance benchmarking for the `{model_name}` model on \"{instance_types}\" instance{plural} on multiple datasets and based on the test results the best price performance for dataset `{ds}` is provided by the `{selected_instance_type}` instance type.  {mkdn_table}\n",
    "\n",
    "The price performance comparison for different instance types is presented below:\n",
    "\n",
    "![Price performance comparison]({business_summary_plot_fpath})\n",
    "\n",
    "The configuration used for these tests is available in the [`config`]({cfg_file_path}) file.\n",
    "\n",
    "The cost to run each experiment is provided in the table below. The total cost for running all experiments is {total_cost_as_str}.\n",
    "\n",
    "{cost_table}\n",
    "\n",
    "\"\"\"\n",
    "transposed_list = []\n",
    "best_instance_type_info = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]\n",
    "del best_instance_type_info[\"score\"]\n",
    "for k,v in best_instance_type_info.items():\n",
    "    transposed_list.append({\"Information\": k, \"Value\": v})\n",
    "mkdn_table = Tomark.table(transposed_list)\n",
    "\n",
    "\n",
    "plural = \"s\" if len(df_summary_metrics.instance_type.unique()) > 1 else \"\"\n",
    "instance_types_md = \", \".join([f\"`{it}`\" for it in df_summary_metrics.instance_type.unique()])\n",
    "selected_instance_type: str = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]['instance_type']\n",
    "ds: str = config['metrics']['dataset_of_interest']\n",
    "\n",
    "business_summary: str = BUSINESS_SUMMARY.format(model_name=config['general']['model_name'],\n",
    "                                              instance_types=instance_types_md,\n",
    "                                              plural=plural,\n",
    "                                              ds=ds,\n",
    "                                              selected_instance_type=selected_instance_type,\n",
    "                                              mkdn_table=\"\\n\" + mkdn_table,\n",
    "                                              cfg_file_path=os.path.basename(CONFIG_FILE),\n",
    "                                              business_summary_plot_fpath=BUSINESS_SUMMARY_PLOT_FNAME,\n",
    "                                              cost_table=cost_mkdn_table,\n",
    "                                              total_cost_as_str=f\"${df_cost_metrics.cost.sum():.2f}\"\n",
    "                                              )\n",
    "business_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "dttm = str(datetime.utcnow())\n",
    "datasets_used = \", \".join([f\"`{d}`\" for d in config['s3_read_data']['source_data_files']])\n",
    "overall_results_md = OVERALL_RESULTS_MD.format(dttm=dttm,\n",
    "                                               business_summary=business_summary,\n",
    "                                               datasets=datasets_used)\n",
    "\n",
    "results_group_cols: List[str] = ['instance_type', 'payload_file']\n",
    "result_rows: List[str] = []\n",
    "for row in df_summary_metrics[results_group_cols].drop_duplicates().iterrows():\n",
    "    instance_type = row[1]['instance_type']\n",
    "    dataset = row[1]['payload_file']\n",
    "    df_summary_metrics_nz_subset = df_summary_metrics_nz[(df_summary_metrics_nz.instance_type == instance_type) &\n",
    "                                                          (df_summary_metrics_nz.payload_file == dataset) &\n",
    "                                                           (df_summary_metrics_nz.latency_mean <= LATENCY_BUDGET)]\n",
    "    num_results = df_summary_metrics_nz_subset.shape[0]\n",
    "    result_row: Optional[str] = None\n",
    "    if num_results > 0:\n",
    "        logger.info(f\"there are {num_results} options to choose the best option from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        df_summary_metrics_nz_subset_selected = df_summary_metrics_nz_subset[df_summary_metrics_nz_subset.concurrency == df_summary_metrics_nz_subset.concurrency.max()]\n",
    "        best = df_summary_metrics_nz_subset_selected.to_dict(orient='records')[0]\n",
    "        # logger.info(best)\n",
    "        result_desc = RESULT_DESC.format(latency_budget=LATENCY_BUDGET,\n",
    "                           instance_type=best['instance_type'],\n",
    "                           dataset=dataset,\n",
    "                           concurrency=best['concurrency'],\n",
    "                           latency_mean=best['latency_mean'],\n",
    "                           prompt_size=int(best['prompt_token_count_mean']),\n",
    "                           completion_size=int(best['completion_token_count_mean']),\n",
    "                           tpm=int(best['transactions_per_minute']))     \n",
    "        \n",
    "        # logger.info(result_desc)\n",
    "    else:\n",
    "        logger.info(f\"there are NO options to choose from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        result_desc = RESULT_FAILURE_DESC.format(latency_budget=LATENCY_BUDGET,\n",
    "                           instance_type=best['instance_type'],\n",
    "                           dataset=dataset)\n",
    "    result_row: str = RESULT_ROW.format(instance_type=best['instance_type'],\n",
    "                                        dataset=dataset,\n",
    "                                        desc=result_desc)\n",
    "    result_rows.append(result_row)\n",
    "        \n",
    "    \n",
    "    #logger.info(f\"instance_type={row[0]}, payload_file={row[1]}\")\n",
    "overall_results_md += \"\\n\".join(result_rows)\n",
    "\n",
    "OVERALL_RESULTS_PLOTS_MD: str = \"\"\"\n",
    "\n",
    "## Plots\n",
    "\n",
    "The following plots provide insights into the results from the different experiments run.\n",
    "\n",
    "![{plot1_text}]({plot1_fname})\n",
    "\n",
    "![{plot2_text}]({plot2_fname})\n",
    "\n",
    "![{plot3_text}]({plot3_fname})\n",
    "\"\"\"\n",
    "\n",
    "overall_results_plots_md: str = OVERALL_RESULTS_PLOTS_MD.format(plot1_text=ERROR_RATES_PLOT_TEXT, \n",
    "                                                                plot1_fname=ERROR_RATES_PLOT_FNAME,\n",
    "                                                                plot2_text=TOKENS_VS_LATENCY_PLOT_TEXT, \n",
    "                                                                plot2_fname=TOKENS_VS_LATENCY_PLOT_FNAME,\n",
    "                                                                plot3_text=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT, \n",
    "                                                                plot3_fname=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "\n",
    "overall_results_md += overall_results_plots_md\n",
    "\n",
    "fpath: str = os.path.join(METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"writing final markdown to {METRICS_DIR}\")\n",
    "Path(fpath).write_text(overall_results_md)\n",
    "logger.info(overall_results_md)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_results_md, BUCKET_NAME, \"\", METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"results.md file saved to to s3://{BUCKET_NAME}/{METRICS_DIR}/{RESULTS_DESC_MD_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the metrics and report files locally\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "logger.info(f\"going to download all metrics and reports from s3 into {RESULTS_DIR} directory\")\n",
    "download_multiple_files_from_s3(BUCKET_NAME, METRICS_DIR, RESULTS_DIR)\n",
    "import glob\n",
    "result_files = glob.glob(os.path.join(RESULTS_DIR, \"**\"), recursive=True)\n",
    "logger.info(\"\\n\".join([f for f in result_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get information about models in Amazon SageMaker JumpStart.\n",
    "Tested with sagemaker SDK version 2.203.0.\n",
    "\"\"\"\n",
    "\n",
    "## View the full list of models available through JumpStart here: https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# global constants\n",
    "try:\n",
    "    ROLE_ARN: str = get_execution_role()\n",
    "except Exception as e:\n",
    "    # if not running this code on SageMaker, set the IAM role ARN to use here\n",
    "    ROLE_ARN: str = \"arn:aws:iam::<your-account-id>:role/<role-name>\"\n",
    "\n",
    "# models of interest\n",
    "JS_MODEL_ID_LIST: List = ['meta-textgenerationneuron-llama-2-70b-f']\n",
    "\n",
    "## Add pricing as needed, using the instances\n",
    "\n",
    "## These are per-hour prices for us-east-1 below, \n",
    "## PLEASE CONSULT Amazon SageMaker Pricing page https://aws.amazon.com/sagemaker/pricing/ for up to date information.\n",
    "PRICING_PER_INSTANCE: Dict = {\n",
    "    \"ml.g5.2xlarge\": 1.515,\n",
    "    \"ml.p4de.24xlarge\": 40.966,\n",
    "    \"ml.g5.12xlarge\": 7.09,\n",
    "    \"ml.g5.24xlarge\": 10.18,\n",
    "    \"ml.g5.48xlarge\": 20.36,\n",
    "    \"ml.inf2.24xlarge\": 7.79,\n",
    "    \"ml.inf2.48xlarge\": 15.58,\n",
    "    \"ml.p4d.24xlarge\": 37.688,\n",
    "    \"ml.p3.2xlarge\": 3.825,\n",
    "    \"ml.m5.xlarge\": 0.23\n",
    "}\n",
    "## convert the pricing information into a DF\n",
    "pricing_df = pd.DataFrame(list(PRICING_PER_INSTANCE.items()), columns=['instance_type', 'price_per_hour'])\n",
    "\n",
    "JS_MODEL_INFO_CSV_FNAME: str = \"jumpstart_models.csv\"\n",
    "print(f\"SageMaker execution role ARN being used -> {ROLE_ARN}\")\n",
    "\n",
    "def get_model_info(model_id: str, model_version: str = \"*\") -> Dict:\n",
    "    \"\"\"\n",
    "    Get information about the model that Amazon SageMaker JumpStart would use to deploy it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to create a JumpStartModel instance if not then catch the non jumpstart model\n",
    "        my_model: JumpStartModel = JumpStartModel(model_id=model_id, model_version=model_version, role=ROLE_ARN)\n",
    "        # Initialize model configurations\n",
    "        model_config = vars(my_model)\n",
    "        print(json.dumps(model_config, indent=2, default=str))\n",
    "        instance_type = model_config.get('instance_type')\n",
    "        \n",
    "        print(f\"model_id={model_id}, model_version={model_version}, recommended instance_type={instance_type}\")\n",
    "        return dict(model_id=model_id, model_version=model_version, instance_type=instance_type, model_config=model_config)         \n",
    "    except Exception as e:\n",
    "        # Handle the case where the model is not part of JumpStart\n",
    "        print(f\"Model Id {model_id} is currently not supported by JumpStart\")\n",
    "        return dict(model_id=model_id, model_version=model_version, instance_type=None, model_config=None)\n",
    "    \n",
    "model_info = list(map(get_model_info, JS_MODEL_ID_LIST))\n",
    "\n",
    "# Load the list into a dataframe\n",
    "df = pd.DataFrame(model_info)\n",
    "df = df.sort_values(by=\"model_id\")\n",
    "print(df)\n",
    "\n",
    "# Merge df with pricing_df on 'instance_type'\n",
    "merged_df = pd.merge(df, pricing_df, on='instance_type', how='left')\n",
    "\n",
    "first_few_cols = ['model_id', 'model_version', 'instance_type', 'price_per_hour']\n",
    "col_order = first_few_cols +  [col for col in merged_df.columns if col not in first_few_cols]\n",
    "final_df = merged_df[col_order]\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "## display it in the final csv\n",
    "#final_df.to_csv(JS_MODEL_INFO_CSV_FNAME, index=False)\n",
    "#print(f\"CSV file with merged model info and pricing of shape {final_df.shape} saved at: {JS_MODEL_INFO_CSV_FNAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
