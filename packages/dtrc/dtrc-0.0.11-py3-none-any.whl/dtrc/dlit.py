# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/70_dlit.ipynb.

# %% auto 0
__all__ = ['tstep_forward', 'Learner']

# %% ../nbs/70_dlit.ipynb 6
#| export


# %% ../nbs/70_dlit.ipynb 8
from types import FunctionType

# %% ../nbs/70_dlit.ipynb 11
#| export

# %% ../nbs/70_dlit.ipynb 13
try: import numpy as np
except ImportError: ...

try: import pandas as pd
except ImportError: ...

try: import matplotlib.pyplot as plt
except ImportError: ...

try: import seaborn as sns
except ImportError: ...


# %% ../nbs/70_dlit.ipynb 15
try: import torch
except ImportError: ...

try: import torch, torch.nn as nn
except: ...

try: from torch.utils.data import Dataset, DataLoader
except: ...

try: import lightning as L
except: ...

try: from lightning import LightningDataModule
except: ...

# %% ../nbs/70_dlit.ipynb 17
from nlit import LABEL, MPS
from chck import iscall, notnone, istensor
from sigr import get_signature_defaults
from quac import ptdevice, tensor
from utrc.atyp import P, LossFunction
from utrc.misc import get_categories, grouplens, drop_column, generate_steps
from utrc.smpl import categorical_sample
from utrc.data import datasplit

from etrc import Stage, BatchReturn, DataFormat

# %% ../nbs/70_dlit.ipynb 19
from .cons import DATASET_CLS, DATAFRAME_FN
from .util import DataSplits
from .make import diamonds
from .dset import PandasDataset, PandasCategoricalDataset, PandasTimeSeriesDataset

# %% ../nbs/70_dlit.ipynb 27
def tstep_forward(x: torch.Tensor, model: L.LightningModule) -> list[float]:
    tdif = 1
    tend = x.size(0)
    steps = generate_steps(1, tdif, tend)
    losses = []

    # (0, 1), (1, 2), (2, 3), ....
    for (t0, tn) in steps:
        # NOTE: some ODE solver's do not like this / it will take a long time
        # as t, will automatically be split into 100 steps
        # step = torch.tensor([t0, t1])
        
        # NOTE: creating a single step this way, however, is fine
        step = torch.linspace(t0, tn, (tn + 1) - t0)
        tout, yout = model(x[t0], step)
        yend = yout.size(0) # number of integration steps
        yrng = range(t0, t0+yend) # ideally, t0, ti, ..., tj, tn
        for i, t in enumerate(yrng):
            if t >= tend or i >= yend: break
            diff = model.crit(x[t] + torch.randn_like(x[t]), yout[i])
            
            # NOTE: if comparing (x[t], y[i+1]), outputs the following:
            # if t >= tend or i + 1 >= yend: continue
            # diff = self.crit(x[t], yout[i+1])
            # print(f'crit(x[{t}], y[{t+1}])')
            # model(x, tensor([0., 1.]))
            # crit(x[0], y[1])
            # model(x, tensor([1., 2.]))
            # crit(x[1], y[2])
            # model(x, tensor([2., 3.]))
            # crit(x[2], y[3])
            # model(x, tensor([3., 4.]))
            # crit(x[3], y[4])
            # model(x, tensor([4., 5.]))
            # crit(x[4], y[5])
            
            # NOTE: if comparing (x[t+1], y[1]), outputs the following:
            # if t + 1 >= tend or i + 1 >= yend: continue
            # diff = self.crit(x[t+1], yout[i])
            # print(f'crit(x[{t+1}], y[{t}])')
            # model(x, tensor([0., 1.]))
            # crit(x[1], y[0])
            # model(x, tensor([1., 2.]))
            # crit(x[2], y[1])
            # model(x, tensor([2., 3.]))
            # crit(x[3], y[2])
            # model(x, tensor([3., 4.]))
            # crit(x[4], y[3])
            # model(x, tensor([4., 5.]))
            
            losses.append(diff)
    return losses

# %% ../nbs/70_dlit.ipynb 28
class Learner(L.LightningModule):
    '''Generic Learner for TorchSDE.'''
    def __init__(
        self, 
        model: L.LightningModule, 
        crit: LossFunction,
        t_step: tensor[int] | list[int] | None = None, 
        device: str | ptdevice = MPS,
        lr: float = 1e-3, 
        wd: float = 1e-5, 
        step_lr: int = 100,
        lr_factor: float = 0.1
    ):
        super().__init__()
        self.model = model.to(device)
        self.crit = crit
        
        self.device
        self.lr = lr
        self.wd = wd
        self.step_lr = step_lr
        self.lr_factor = lr_factor
        
        t_step = getattr(model,  't_step', t_step)
        if notnone(t_step) and not istensor(t_step): t_step = torch.tensor(t_step)
        self.t_step = t_step
        
        
    def forward(self, x, t = None):
        if t is None: 
            t = self.t_step
            # x.to(self.device)
        print(f'model(x, {t})')
        return self.model(x, t)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        
        tdif = 1
        tend = x.size(0)
        steps = generate_steps(1, tdif, tend)
        losses = []

        for (t0, t1) in steps:
            # step = torch.tensor([t0, t1])
            step = torch.linspace(t0, t1, (t1 + 1) - t0)
            tout, yout = self(x[t0], step)
            yend = yout.size(0)
            yrng = range(t0, t0+yend)
            for i, t in enumerate(yrng):
                if t + 1 >= tend or i + 1 >= yend: continue
                diff = self.crit(x[t+1], yout[i])
                print(f'crit(x[{t+1}], y[{t}])')
                losses.append(diff)
        
        # trng = range(x.size(0))
        # loss = torch.stack([self.crit(x[i], yout[i]) for i in trng])
        # loss = torch.mean(loss)
        loss = sum(losses)
        # loss = sum([self.crit(x[i], yout[i]) for i in range(x.size(0))])
        self.log('loss', loss, prog_bar=True)
        return loss
    
    # def validation_step(self, batch, batch_idx):
    #     x, y = batch
    #     tout, yout = self(x[0], self.t_step)
    #     trng = range(x.size(0))
    #     loss = torch.stack([self.crit(x[i], yout[i]) for i in trng])
    #     loss = torch.mean(loss)
    #     # loss = sum([self.crit(x[i], yout[i]) for i in range(x.size(0))])
    #     self.log('val_loss', loss, prog_bar=True)
    #     return loss

    # def test_step(self, batch, batch_idx):
    #     x, y = batch
    #     tout, yout = self(x[0])
    #     loss = torch.stack([self.crit(x[i], yout[i]) for i in range(x.size(0))])
    #     loss = torch.mean(loss)
    #     self.log('test_loss', loss, prog_bar=True)
    #     return loss

    # def predict_step(self, batch):
    #     x, y = batch
    #     tout, yout = self.model(x[0])
    #     return yout

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.wd)
        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=self.lr_factor)
        # scheduler = optim.lr_scheduler.StepLR(optimizer, self.step_lr, gamma=0.99)
        # return dict(optimizer=optimizer, scheduler=scheduler, monitor='val_loss')
        return optimizer
        # return [optimizer], [scheduler]

# %% ../nbs/70_dlit.ipynb 32
#| export

