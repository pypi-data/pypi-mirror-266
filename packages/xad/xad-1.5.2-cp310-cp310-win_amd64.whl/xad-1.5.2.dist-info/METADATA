Metadata-Version: 2.1
Name: xad
Version: 1.5.2
Summary: High-Performance Automatic Differentiation for Python
Home-page: https://auto-differentiation.github.io
License: AGPL-3.0-or-later
Keywords: automatic-differentiation,derivatives,machine-learning,optimisation,numerical-analysis,scientific-computing,risk-management,computer-graphics,robotics,biotechnology,meteorology,quant-finance
Author: Auto Differentiation Dev Team
Author-email: dev@auto-differentiation.com
Requires-Python: >=3.8.1,<4.0
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Financial and Insurance Industry
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)
Classifier: License :: Other/Proprietary License
Classifier: Natural Language :: English
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: POSIX :: Linux
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Software Development
Project-URL: Documentation, https://auto-differentiation.github.io/tutorials/python
Project-URL: Download, https://pypi.org/project/xad/#files
Project-URL: Repository, https://github.com/auto-differentiation/xad-py
Project-URL: Release notes, https://github.com/auto-differentiation/xad-py/releases
Project-URL: Tracker, https://github.com/auto-differentiation/xad-py/issues
Description-Content-Type: text/markdown

[![Python](https://img.shields.io/pypi/pyversions/xad.svg)](https://auto-differentiation.github.io/tutorials/python)


XAD is a library designed for
[automatic differentiation](https://auto-differentiation.github.io/tutorials/aad/),
aimed at both beginners and advanced users. It is intended for use in
production environments, emphasizing performance and ease of use. The library
facilitates the computation of derivatives within computer programs, making
the process efficient and straightforward for a wide range of mathematical
functions, from simple arithmetic to complex calculations, ensuring accurate
and automatic derivative computations.

The Python bindings for XAD offer the following features:

- Support for both forward and adjoint modes at the first order.
- Strong exception-safety guarantees.
- High performance, as demonstrated in extensive production use.

For more details and to integrate XAD into your projects, consult the
comprehensive [documentation](https://auto-differentiation.github.io/tutorials/python).

## Application Areas

Automatic differentiation has many application areas, for example:

-   **Machine Learning and Deep Learning:** Training neural networks or other
    machine learning models.
-   **Optimization:** Solving optimization problems in engineering and finance.
-   **Numerical Analysis:** Enhancing numerical solution methods for
    differential equations.
-   **Scientific Computing:** Simulating physical systems and processes.
-   **Risk Management and Quantitative Finance:** Assessing and hedging risk in
    financial models.
-   **Computer Graphics:** Optimizing rendering algorithms.
-   **Robotics:** Improving control and simulation of robotic systems.
-   **Meteorology:** Enhancing weather prediction models.
-   **Biotechnology:** Modeling biological processes and systems.

## Getting Started

Install:

```text
pip install xad
```


Calculate first-order derivatives in adjoint mode:

```python
import xad.adj_1st as xadj


# set independent variables
x0_ad = xadj.Real(1.0)
x1_ad = xadj.Real(1.5)
x2_ad = xadj.Real(1.3)
x3_ad = xadj.Real(1.2)

with xadj.Tape() as tape:
    # and register them
    tape.registerInput(x0_ad)
    tape.registerInput(x1_ad)
    tape.registerInput(x2_ad)
    tape.registerInput(x3_ad)

    # start recording derivatives
    tape.newRecording()

    # calculate the output
    y = x0_ad + x1_ad - x2_ad * x3_ad

    # register and seed adjoint of output
    tape.registerOutput(y)
    y.derivative = 1.0

    # compute all other adjoints
    tape.computeAdjoints()

    # output results
    print(f"y = {y}")
    print(f"first order derivatives:\n")
    print(f"dy/dx0 = {x0_ad.derivative}")
    print(f"dy/dx1 = {x1_ad.derivative}")
    print(f"dy/dx2 = {x2_ad.derivative}")
    print(f"dy/dx3 = {x3_ad.derivative}")
```

For more information, see the [Documentation](https://auto-differentiation.github.io/tutorials/python).

## Related Projects

- XAD Comprehensive automatic differentiation in [Python](https://github.com/auto-differentiation/xad-py) and [C++](https://github.com/auto-differentiation/xad)
- QuantLib-Risks: Fast risk evaluations in [Python](https://github.com/auto-differentiation/QuantLib-Risks-Py) and [C++](https://github.com/auto-differentiation/QuantLib-Risks-Cpp)

