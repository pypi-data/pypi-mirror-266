# coding: utf-8
# Copyright (c) 2016, 2024, Oracle and/or its affiliates.  All rights reserved.
# This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.

# NOTE: This class is auto generated by OracleSDKGenerator. DO NOT EDIT. API Version: 20231130

from __future__ import absolute_import

from .choice import Choice
from .cohere_llm_inference_request import CohereLlmInferenceRequest
from .cohere_llm_inference_response import CohereLlmInferenceResponse
from .dedicated_serving_mode import DedicatedServingMode
from .embed_text_details import EmbedTextDetails
from .embed_text_result import EmbedTextResult
from .generate_text_details import GenerateTextDetails
from .generate_text_result import GenerateTextResult
from .generated_text import GeneratedText
from .llama_llm_inference_request import LlamaLlmInferenceRequest
from .llama_llm_inference_response import LlamaLlmInferenceResponse
from .llm_inference_request import LlmInferenceRequest
from .llm_inference_response import LlmInferenceResponse
from .logprobs import Logprobs
from .on_demand_serving_mode import OnDemandServingMode
from .serving_mode import ServingMode
from .summarize_text_details import SummarizeTextDetails
from .summarize_text_result import SummarizeTextResult
from .token_likelihood import TokenLikelihood

# Maps type names to classes for generative_ai_inference services.
generative_ai_inference_type_mapping = {
    "Choice": Choice,
    "CohereLlmInferenceRequest": CohereLlmInferenceRequest,
    "CohereLlmInferenceResponse": CohereLlmInferenceResponse,
    "DedicatedServingMode": DedicatedServingMode,
    "EmbedTextDetails": EmbedTextDetails,
    "EmbedTextResult": EmbedTextResult,
    "GenerateTextDetails": GenerateTextDetails,
    "GenerateTextResult": GenerateTextResult,
    "GeneratedText": GeneratedText,
    "LlamaLlmInferenceRequest": LlamaLlmInferenceRequest,
    "LlamaLlmInferenceResponse": LlamaLlmInferenceResponse,
    "LlmInferenceRequest": LlmInferenceRequest,
    "LlmInferenceResponse": LlmInferenceResponse,
    "Logprobs": Logprobs,
    "OnDemandServingMode": OnDemandServingMode,
    "ServingMode": ServingMode,
    "SummarizeTextDetails": SummarizeTextDetails,
    "SummarizeTextResult": SummarizeTextResult,
    "TokenLikelihood": TokenLikelihood
}
