Metadata-Version: 2.1
Name: liah
Version: 0.1.0
Summary: Insert a Lie in a Haystack and evaluate the model's ability to detect it.
Home-page: https://github.com/melvinebenezer/Liah-Lie_in_a_haystack
Author: James Melvin Priyarajan
Author-email: melvinebenezer@gmail.com
License: Apache 2.0
Keywords: llm,needle in a haystack
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: openai>=1.16.1
Requires-Dist: matplotlib
Requires-Dist: tiktoken
Requires-Dist: tqdm


# ðŸ¤¥ LIAH - a Lie-in-haystack [WIP]

![LIAH](/images/liah.png "Liah")

With longer context lengths for LLMs. It is increasingly difficult to test
if fine tuned models attend to all depths of the context.

The needle in haystack is a popular approach. However since the LLMs can also answer
about the needle instead of the needle. Tests have shown that a "Lie" works well in 
this context ðŸ˜Š

## Example Usage

    # update OPENAI_API_KEY in the env with your token. 
    # If you need Open AI models for the final evaluation
    from vllm import LLM, SamplingParams

    # Create a sampling params object.
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=4096)
    # llm = LLM(model="mistralai/Mistral-7B-v0.1")
    llm = LLM(model="meta-llama/Llama-2-70b-hf", tensor_parallel_size=4, max_model_len=1500) # need 4 A100s 40GB
    liah = Liah(max_context_length=2000)
    
    for i, sample in enumerate(liah.getSample()):
        # test the sample text with your model
        output = llm.generate([sample["prompt"], sampling_params)[0]
        liah.update(sample, output.outputs[0].text)
    plotFilePath = liah.evaluate()

## Contribute

    bash
    pip install pre-commit

    then (in the repository, just once)

    bash
    pre-commit install

## before commit (optional)

    bash
    pre-commit run --all-files
