{
	"$id": "/inference/schemas/text-generation/input.json",
	"$schema": "http://json-schema.org/draft-06/schema#",
	"description": "Inputs for Text Generation inference",
	"title": "TextGenerationInput",
	"type": "object",
	"properties": {
		"inputs": {
			"description": "The text to initialize generation with",
			"type": "string"
		},
		"parameters": {
			"description": "Additional inference parameters",
			"$ref": "#/$defs/TextGenerationParameters"
		},
		"stream": {
			"description": "Whether to stream output tokens",
			"type": "boolean"
		}
	},
	"$defs": {
		"TextGenerationParameters": {
			"title": "TextGenerationParameters",
			"description": "Additional inference parameters for Text Generation",
			"type": "object",
			"properties": {
				"best_of": {
					"type": "integer",
					"description": "The number of sampling queries to run. Only the best one (in terms of total logprob) will be returned."
				},
				"decoder_input_details": {
					"type": "boolean",
					"description": "Whether or not to output decoder input details"
				},
				"details": {
					"type": "boolean",
					"description": "Whether or not to output details"
				},
				"do_sample": {
					"type": "boolean",
					"description": "Whether to use logits sampling instead of greedy decoding when generating new tokens."
				},
				"max_new_tokens": {
					"type": "integer",
					"description": "The maximum number of tokens to generate."
				},
				"repetition_penalty": {
					"type": "number",
					"description": "The parameter for repetition penalty. A value of 1.0 means no penalty. See [this paper](https://hf.co/papers/1909.05858) for more details."
				},
				"return_full_text": {
					"type": "boolean",
					"description": "Whether to prepend the prompt to the generated text."
				},
				"seed": {
					"type": "integer",
					"description": "The random sampling seed."
				},
				"stop_sequences": {
					"type": "array",
					"items": {
						"type": "string"
					},
					"description": "Stop generating tokens if a member of `stop_sequences` is generated."
				},
				"temperature": {
					"type": "number",
					"description": "The value used to modulate the logits distribution."
				},
				"top_k": {
					"type": "integer",
					"description": "The number of highest probability vocabulary tokens to keep for top-k-filtering."
				},
				"top_p": {
					"type": "number",
					"description": "If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation."
				},
				"truncate": {
					"type": "integer",
					"description": "Truncate input tokens to the given size."
				},
				"typical_p": {
					"type": "number",
					"description": "Typical Decoding mass. See [Typical Decoding for Natural Language Generation](https://hf.co/papers/2202.00666) for more information"
				},
				"watermark": {
					"type": "boolean",
					"description": "Watermarking with [A Watermark for Large Language Models](https://hf.co/papers/2301.10226)"
				}
			}
		}
	},
	"required": ["inputs"]
}
