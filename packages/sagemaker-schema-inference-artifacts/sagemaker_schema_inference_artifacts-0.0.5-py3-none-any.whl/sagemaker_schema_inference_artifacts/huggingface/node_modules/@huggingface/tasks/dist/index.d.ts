declare const MODALITIES: readonly ["cv", "nlp", "audio", "tabular", "multimodal", "rl", "other"];
type Modality = (typeof MODALITIES)[number];
declare const MODALITY_LABELS: {
    multimodal: string;
    nlp: string;
    audio: string;
    cv: string;
    rl: string;
    tabular: string;
    other: string;
};
/**
 * Public interface for a sub task.
 *
 * This can be used in a model card's `model-index` metadata.
 * and is more granular classification that can grow significantly
 * over time as new tasks are added.
 */
interface SubTask {
    /**
     * type of the task (e.g. audio-source-separation)
     */
    type: string;
    /**
     * displayed name of the task (e.g. Audio Source Separation)
     */
    name: string;
}
/**
 * Public interface for a PipelineData.
 *
 * This information corresponds to a pipeline type (aka task)
 * in the Hub.
 */
interface PipelineData {
    /**
     * displayed name of the task (e.g. Text Classification)
     */
    name: string;
    subtasks?: SubTask[];
    modality: Modality;
    /**
     * color for the tag icon.
     */
    color: "blue" | "green" | "indigo" | "orange" | "red" | "yellow";
    /**
     * whether to hide in /models filters
     */
    hideInModels?: boolean;
    /**
     * whether to hide in /datasets filters
     */
    hideInDatasets?: boolean;
}
declare const PIPELINE_DATA: {
    "text-classification": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "orange";
    };
    "token-classification": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "blue";
    };
    "table-question-answering": {
        name: string;
        modality: "nlp";
        color: "green";
    };
    "question-answering": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "blue";
    };
    "zero-shot-classification": {
        name: string;
        modality: "nlp";
        color: "yellow";
    };
    translation: {
        name: string;
        modality: "nlp";
        color: "green";
    };
    summarization: {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "indigo";
    };
    "feature-extraction": {
        name: string;
        modality: "nlp";
        color: "red";
    };
    "text-generation": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "indigo";
    };
    "text2text-generation": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "indigo";
    };
    "fill-mask": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "red";
    };
    "sentence-similarity": {
        name: string;
        modality: "nlp";
        color: "yellow";
    };
    "text-to-speech": {
        name: string;
        modality: "audio";
        color: "yellow";
    };
    "text-to-audio": {
        name: string;
        modality: "audio";
        color: "yellow";
    };
    "automatic-speech-recognition": {
        name: string;
        modality: "audio";
        color: "yellow";
    };
    "audio-to-audio": {
        name: string;
        modality: "audio";
        color: "blue";
    };
    "audio-classification": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "audio";
        color: "green";
    };
    "voice-activity-detection": {
        name: string;
        modality: "audio";
        color: "red";
    };
    "depth-estimation": {
        name: string;
        modality: "cv";
        color: "yellow";
    };
    "image-classification": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "cv";
        color: "blue";
    };
    "object-detection": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "cv";
        color: "yellow";
    };
    "image-segmentation": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "cv";
        color: "green";
    };
    "text-to-image": {
        name: string;
        modality: "cv";
        color: "yellow";
    };
    "image-to-text": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "cv";
        color: "red";
    };
    "image-to-image": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "cv";
        color: "indigo";
    };
    "image-to-video": {
        name: string;
        modality: "cv";
        color: "indigo";
    };
    "unconditional-image-generation": {
        name: string;
        modality: "cv";
        color: "green";
    };
    "video-classification": {
        name: string;
        modality: "cv";
        color: "blue";
    };
    "reinforcement-learning": {
        name: string;
        modality: "rl";
        color: "red";
    };
    robotics: {
        name: string;
        modality: "rl";
        subtasks: {
            type: string;
            name: string;
        }[];
        color: "blue";
    };
    "tabular-classification": {
        name: string;
        modality: "tabular";
        subtasks: {
            type: string;
            name: string;
        }[];
        color: "blue";
    };
    "tabular-regression": {
        name: string;
        modality: "tabular";
        subtasks: {
            type: string;
            name: string;
        }[];
        color: "blue";
    };
    "tabular-to-text": {
        name: string;
        modality: "tabular";
        subtasks: {
            type: string;
            name: string;
        }[];
        color: "blue";
        hideInModels: true;
    };
    "table-to-text": {
        name: string;
        modality: "nlp";
        color: "blue";
        hideInModels: true;
    };
    "multiple-choice": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "blue";
        hideInModels: true;
    };
    "text-retrieval": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "nlp";
        color: "indigo";
        hideInModels: true;
    };
    "time-series-forecasting": {
        name: string;
        modality: "tabular";
        subtasks: {
            type: string;
            name: string;
        }[];
        color: "blue";
        hideInModels: true;
    };
    "text-to-video": {
        name: string;
        modality: "cv";
        color: "green";
    };
    "image-text-to-text": {
        name: string;
        modality: "multimodal";
        color: "red";
        hideInDatasets: true;
    };
    "visual-question-answering": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "multimodal";
        color: "red";
    };
    "document-question-answering": {
        name: string;
        subtasks: {
            type: string;
            name: string;
        }[];
        modality: "multimodal";
        color: "blue";
        hideInDatasets: true;
    };
    "zero-shot-image-classification": {
        name: string;
        modality: "cv";
        color: "yellow";
    };
    "graph-ml": {
        name: string;
        modality: "other";
        color: "green";
    };
    "mask-generation": {
        name: string;
        modality: "cv";
        color: "indigo";
    };
    "zero-shot-object-detection": {
        name: string;
        modality: "cv";
        color: "yellow";
    };
    "text-to-3d": {
        name: string;
        modality: "cv";
        color: "yellow";
    };
    "image-to-3d": {
        name: string;
        modality: "cv";
        color: "green";
    };
    "image-feature-extraction": {
        name: string;
        modality: "cv";
        color: "indigo";
    };
    other: {
        name: string;
        modality: "other";
        color: "blue";
        hideInModels: true;
        hideInDatasets: true;
    };
};
type PipelineType = keyof typeof PIPELINE_DATA;
type WidgetType = PipelineType | "conversational";
declare const PIPELINE_TYPES: ("other" | "text-classification" | "token-classification" | "table-question-answering" | "question-answering" | "zero-shot-classification" | "translation" | "summarization" | "feature-extraction" | "text-generation" | "text2text-generation" | "fill-mask" | "sentence-similarity" | "text-to-speech" | "text-to-audio" | "automatic-speech-recognition" | "audio-to-audio" | "audio-classification" | "voice-activity-detection" | "depth-estimation" | "image-classification" | "object-detection" | "image-segmentation" | "text-to-image" | "image-to-text" | "image-to-image" | "image-to-video" | "unconditional-image-generation" | "video-classification" | "reinforcement-learning" | "robotics" | "tabular-classification" | "tabular-regression" | "tabular-to-text" | "table-to-text" | "multiple-choice" | "text-retrieval" | "time-series-forecasting" | "text-to-video" | "image-text-to-text" | "visual-question-answering" | "document-question-answering" | "zero-shot-image-classification" | "graph-ml" | "mask-generation" | "zero-shot-object-detection" | "text-to-3d" | "image-to-3d" | "image-feature-extraction")[];
declare const SUBTASK_TYPES: string[];
declare const PIPELINE_TYPES_SET: Set<"other" | "text-classification" | "token-classification" | "table-question-answering" | "question-answering" | "zero-shot-classification" | "translation" | "summarization" | "feature-extraction" | "text-generation" | "text2text-generation" | "fill-mask" | "sentence-similarity" | "text-to-speech" | "text-to-audio" | "automatic-speech-recognition" | "audio-to-audio" | "audio-classification" | "voice-activity-detection" | "depth-estimation" | "image-classification" | "object-detection" | "image-segmentation" | "text-to-image" | "image-to-text" | "image-to-image" | "image-to-video" | "unconditional-image-generation" | "video-classification" | "reinforcement-learning" | "robotics" | "tabular-classification" | "tabular-regression" | "tabular-to-text" | "table-to-text" | "multiple-choice" | "text-retrieval" | "time-series-forecasting" | "text-to-video" | "image-text-to-text" | "visual-question-answering" | "document-question-answering" | "zero-shot-image-classification" | "graph-ml" | "mask-generation" | "zero-shot-object-detection" | "text-to-3d" | "image-to-3d" | "image-feature-extraction">;

/**
 * See default-widget-inputs.ts for the default widget inputs, this files only contains the types
 */
type TableData = Record<string, (string | number)[]>;
type WidgetExampleOutputLabels = Array<{
    label: string;
    score: number;
}>;
interface WidgetExampleOutputAnswerScore {
    answer: string;
    score: number;
}
interface WidgetExampleOutputText {
    text: string;
}
interface WidgetExampleOutputUrl {
    url: string;
}
type WidgetExampleOutput = WidgetExampleOutputLabels | WidgetExampleOutputAnswerScore | WidgetExampleOutputText | WidgetExampleOutputUrl;
interface WidgetExampleBase<TOutput> {
    example_title?: string;
    group?: string;
    /**
     * Potential overrides to API parameters for this specific example
     * (takes precedences over the model card metadata's inference.parameters)
     */
    parameters?: {
        aggregation_strategy?: string;
        top_k?: number;
        top_p?: number;
        temperature?: number;
        max_new_tokens?: number;
        do_sample?: boolean;
        negative_prompt?: string;
        guidance_scale?: number;
        num_inference_steps?: number;
    };
    /**
     * Optional output
     */
    output?: TOutput;
}
interface ChatMessage {
    role: "user" | "assistant" | "system";
    content: string;
}
interface WidgetExampleChatInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    messages: ChatMessage[];
}
interface WidgetExampleTextInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    text: string;
}
interface WidgetExampleTextAndContextInput<TOutput = WidgetExampleOutput> extends WidgetExampleTextInput<TOutput> {
    context: string;
}
interface WidgetExampleTextAndTableInput<TOutput = WidgetExampleOutput> extends WidgetExampleTextInput<TOutput> {
    table: TableData;
}
interface WidgetExampleAssetInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    src: string;
}
interface WidgetExampleAssetAndPromptInput<TOutput = WidgetExampleOutput> extends WidgetExampleAssetInput<TOutput> {
    prompt: string;
}
type WidgetExampleAssetAndTextInput<TOutput = WidgetExampleOutput> = WidgetExampleAssetInput<TOutput> & WidgetExampleTextInput<TOutput>;
type WidgetExampleAssetAndZeroShotInput<TOutput = WidgetExampleOutput> = WidgetExampleAssetInput<TOutput> & WidgetExampleZeroShotTextInput<TOutput>;
interface WidgetExampleStructuredDataInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    structured_data: TableData;
}
interface WidgetExampleTableDataInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    table: TableData;
}
interface WidgetExampleZeroShotTextInput<TOutput = WidgetExampleOutput> extends WidgetExampleTextInput<TOutput> {
    text: string;
    candidate_labels: string;
    multi_class: boolean;
}
interface WidgetExampleSentenceSimilarityInput<TOutput = WidgetExampleOutput> extends WidgetExampleBase<TOutput> {
    source_sentence: string;
    sentences: string[];
}
type WidgetExample<TOutput = WidgetExampleOutput> = WidgetExampleChatInput<TOutput> | WidgetExampleTextInput<TOutput> | WidgetExampleTextAndContextInput<TOutput> | WidgetExampleTextAndTableInput<TOutput> | WidgetExampleAssetInput<TOutput> | WidgetExampleAssetAndPromptInput<TOutput> | WidgetExampleAssetAndTextInput<TOutput> | WidgetExampleAssetAndZeroShotInput<TOutput> | WidgetExampleStructuredDataInput<TOutput> | WidgetExampleTableDataInput<TOutput> | WidgetExampleZeroShotTextInput<TOutput> | WidgetExampleSentenceSimilarityInput<TOutput>;
type KeysOfUnion<T> = T extends unknown ? keyof T : never;
type WidgetExampleAttribute = KeysOfUnion<WidgetExample>;

declare const SPECIAL_TOKENS_ATTRIBUTES: readonly ["bos_token", "eos_token", "unk_token", "sep_token", "pad_token", "cls_token", "mask_token"];
/**
 * Public interface for a tokenizer's special tokens mapping
 */
interface AddedToken {
    __type: "AddedToken";
    content?: string;
    lstrip?: boolean;
    normalized?: boolean;
    rstrip?: boolean;
    single_word?: boolean;
}
type SpecialTokensMap = {
    [key in (typeof SPECIAL_TOKENS_ATTRIBUTES)[number]]?: string | AddedToken | null;
};
/**
 * Public interface for tokenizer config
 */
interface TokenizerConfig extends SpecialTokensMap {
    use_default_system_prompt?: boolean;
    chat_template?: string;
}

declare enum InferenceDisplayability {
    /**
     * Yes
     */
    Yes = "Yes",
    /**
     * And then, all the possible reasons why it's no:
     */
    ExplicitOptOut = "ExplicitOptOut",
    CustomCode = "CustomCode",
    LibraryNotDetected = "LibraryNotDetected",
    PipelineNotDetected = "PipelineNotDetected",
    PipelineLibraryPairNotSupported = "PipelineLibraryPairNotSupported"
}
/**
 * Public interface for model metadata
 */
interface ModelData {
    /**
     * id of model (e.g. 'user/repo_name')
     */
    id: string;
    /**
     * Kept for backward compatibility
     */
    modelId?: string;
    /**
     * Whether or not to enable inference widget for this model
     */
    inference: InferenceDisplayability;
    /**
     * is this model private?
     */
    private?: boolean;
    /**
     * this dictionary has useful information about the model configuration
     */
    config?: {
        architectures?: string[];
        /**
         * Dict of AutoModel or Auto… class name to local import path in the repo
         */
        auto_map?: {
            /**
             * String Property
             */
            [x: string]: string;
        };
        model_type?: string;
        quantization_config?: {
            bits?: number;
            load_in_4bit?: boolean;
            load_in_8bit?: boolean;
        };
        tokenizer_config?: TokenizerConfig;
        adapter_transformers?: {
            model_name?: string;
            model_class?: string;
        };
        diffusers?: {
            _class_name?: string;
        };
        sklearn?: {
            model?: {
                file?: string;
            };
            model_format?: string;
        };
        speechbrain?: {
            speechbrain_interface?: string;
            vocoder_interface?: string;
            vocoder_model_id?: string;
        };
        peft?: {
            base_model_name_or_path?: string;
            task_type?: string;
        };
    };
    /**
     * all the model tags
     */
    tags?: string[];
    /**
     * transformers-specific info to display in the code sample.
     */
    transformersInfo?: TransformersInfo;
    /**
     * Pipeline type
     */
    pipeline_tag?: PipelineType | undefined;
    /**
     * for relevant models, get mask token
     */
    mask_token?: string | undefined;
    /**
     * Example data that will be fed into the widget.
     *
     * can be set in the model card metadata (under `widget`),
     * or by default in `DefaultWidget.ts`
     */
    widgetData?: WidgetExample[] | undefined;
    /**
     * Parameters that will be used by the widget when calling Inference API (serverless)
     * https://huggingface.co/docs/api-inference/detailed_parameters
     *
     * can be set in the model card metadata (under `inference/parameters`)
     * Example:
     * inference:
     *     parameters:
     *         key: val
     */
    cardData?: {
        inference?: boolean | {
            parameters?: Record<string, unknown>;
        };
        base_model?: string | string[];
    };
    /**
     * Library name
     * Example: transformers, SpeechBrain, Stanza, etc.
     */
    library_name?: string;
}
/**
 * transformers-specific info to display in the code sample.
 */
interface TransformersInfo {
    /**
     * e.g. AutoModelForSequenceClassification
     */
    auto_model: string;
    /**
     * if set in config.json's auto_map
     */
    custom_class?: string;
    /**
     * e.g. text-classification
     */
    pipeline_tag?: PipelineType;
    /**
     * e.g. "AutoTokenizer" | "AutoFeatureExtractor" | "AutoProcessor"
     */
    processor?: string;
}

/**
 * This file contains the (simplified) types used
 * to represent queries that are made to Elastic
 * in order to count number of model downloads
 *
 * Read this doc about download stats on the Hub:
 *
 * https://huggingface.co/docs/hub/models-download-stats
 *
 * see also:
 * https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html
 */
type ElasticBoolQueryFilter = {
    term?: {
        path: string;
    };
} | {
    terms?: {
        path: string[];
    };
} | {
    wildcard?: {
        path: string;
    };
};

/**
 * Elements configurable by a model library.
 */
interface LibraryUiElement {
    /**
     * Pretty name of the library.
     * displayed in tags, and on the main
     * call-to-action button on the model page.
     */
    prettyLabel: string;
    /**
     * Repo name of the library's (usually on GitHub) code repo
     */
    repoName: string;
    /**
     * URL to library's (usually on GitHub) code repo
     */
    repoUrl: string;
    /**
     * URL to library's docs
     */
    docsUrl?: string;
    /**
     * Code snippet(s) displayed on model page
     */
    snippets?: (model: ModelData) => string[];
    /**
     * Elastic query used to count this library's model downloads
     *
     * By default, those files are counted:
     * "config.json", "config.yaml", "hyperparams.yaml", "meta.yaml"
     */
    countDownloads?: ElasticBoolQueryFilter;
    /**
     * should we display this library in hf.co/models filter
     * (only for popular libraries with > 100 models)
     */
    filter?: boolean;
}
/**
 * Add your new library here.
 *
 * This is for modeling (= architectures) libraries, not for file formats (like ONNX, etc).
 * (unlike libraries, file formats live in an enum inside the internal codebase.)
 *
 * Doc on how to add a library to the Hub:
 *
 * https://huggingface.co/docs/hub/models-adding-libraries
 *
 * /!\ IMPORTANT
 *
 * The key you choose is the tag your models have in their library_name on the Hub.
 */
declare const MODEL_LIBRARIES_UI_ELEMENTS: {
    "adapter-transformers": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    allennlp: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    asteroid: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    audiocraft: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: false;
    };
    bertopic: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    diffusers: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    doctr: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
    };
    espnet: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    fairseq: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    fastai: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    fasttext: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    flair: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    gliner: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: false;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    grok: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        filter: false;
        countDownloads: {
            terms: {
                path: string[];
            };
        };
    };
    keras: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    k2: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
    };
    mindspore: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
    };
    "ml-agents": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    mlx: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    "mlx-image": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: false;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    nemo: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    open_clip: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    paddlenlp: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    peft: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    "pyannote-audio": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    pythae: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    "sample-factory": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    "sentence-transformers": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    setfit: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    sklearn: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    spacy: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    "span-marker": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    speechbrain: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    "stable-baselines3": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    stanza: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            term: {
                path: string;
            };
        };
    };
    tensorflowtts: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: (model: ModelData) => string[];
    };
    timm: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
        countDownloads: {
            terms: {
                path: string[];
            };
        };
    };
    transformers: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    "transformers.js": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        docsUrl: string;
        snippets: (model: ModelData) => string[];
        filter: true;
    };
    "unity-sentis": {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        snippets: () => string[];
        filter: true;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
    whisperkit: {
        prettyLabel: string;
        repoName: string;
        repoUrl: string;
        countDownloads: {
            wildcard: {
                path: string;
            };
        };
    };
};
type ModelLibraryKey = keyof typeof MODEL_LIBRARIES_UI_ELEMENTS;
declare const ALL_MODEL_LIBRARY_KEYS: ("sklearn" | "adapter-transformers" | "allennlp" | "asteroid" | "audiocraft" | "bertopic" | "diffusers" | "doctr" | "espnet" | "fairseq" | "fastai" | "fasttext" | "flair" | "gliner" | "grok" | "keras" | "k2" | "mindspore" | "ml-agents" | "mlx" | "mlx-image" | "nemo" | "open_clip" | "paddlenlp" | "peft" | "pyannote-audio" | "pythae" | "sample-factory" | "sentence-transformers" | "setfit" | "spacy" | "span-marker" | "speechbrain" | "stable-baselines3" | "stanza" | "tensorflowtts" | "timm" | "transformers" | "transformers.js" | "unity-sentis" | "whisperkit")[];
declare const ALL_DISPLAY_MODEL_LIBRARY_KEYS: ("sklearn" | "adapter-transformers" | "allennlp" | "asteroid" | "audiocraft" | "bertopic" | "diffusers" | "doctr" | "espnet" | "fairseq" | "fastai" | "fasttext" | "flair" | "gliner" | "grok" | "keras" | "k2" | "mindspore" | "ml-agents" | "mlx" | "mlx-image" | "nemo" | "open_clip" | "paddlenlp" | "peft" | "pyannote-audio" | "pythae" | "sample-factory" | "sentence-transformers" | "setfit" | "spacy" | "span-marker" | "speechbrain" | "stable-baselines3" | "stanza" | "tensorflowtts" | "timm" | "transformers" | "transformers.js" | "unity-sentis" | "whisperkit")[];

/**
 * Mapping from library name (excluding Transformers) to its supported tasks.
 * Inference API (serverless) should be disabled for all other (library, task) pairs beyond this mapping.
 * As an exception, we assume Transformers supports all inference tasks.
 * This mapping is generated automatically by "python-api-export-tasks" action in huggingface/api-inference-community repo upon merge.
 * Ref: https://github.com/huggingface/api-inference-community/pull/158
 */
declare const LIBRARY_TASK_MAPPING_EXCLUDING_TRANSFORMERS: Partial<Record<ModelLibraryKey, PipelineType[]>>;

type PerLanguageMapping = Map<WidgetType, string[] | WidgetExample[]>;
declare const MAPPING_DEFAULT_WIDGET: Map<string, PerLanguageMapping>;

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Audio Classification inference
 */
interface AudioClassificationInput {
    /**
     * The input audio data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: AudioClassificationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Audio Classification
 */
interface AudioClassificationParameters {
    function_to_apply?: ClassificationOutputTransform$3;
    /**
     * When specified, limits the output to the top K most probable classes.
     */
    top_k?: number;
    [property: string]: unknown;
}
/**
 * The function to apply to the model outputs in order to retrieve the scores.
 */
type ClassificationOutputTransform$3 = "sigmoid" | "softmax" | "none";
type AudioClassificationOutput = AudioClassificationOutputElement[];
/**
 * Outputs for Audio Classification inference
 */
interface AudioClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Automatic Speech Recognition inference
 */
interface AutomaticSpeechRecognitionInput {
    /**
     * The input audio data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: AutomaticSpeechRecognitionParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Automatic Speech Recognition
 */
interface AutomaticSpeechRecognitionParameters {
    /**
     * Parametrization of the text generation process
     */
    generate?: GenerationParameters$2;
    /**
     * Whether to output corresponding timestamps with the generated text
     */
    return_timestamps?: boolean;
    [property: string]: unknown;
}
/**
 * Parametrization of the text generation process
 *
 * Ad-hoc parametrization of the text generation process
 */
interface GenerationParameters$2 {
    /**
     * Whether to use sampling instead of greedy decoding when generating new tokens.
     */
    do_sample?: boolean;
    /**
     * Controls the stopping condition for beam-based methods.
     */
    early_stopping?: EarlyStoppingUnion$2;
    /**
     * If set to float strictly between 0 and 1, only tokens with a conditional probability
     * greater than epsilon_cutoff will be sampled. In the paper, suggested values range from
     * 3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language
     * Model Desmoothing](https://hf.co/papers/2210.15191) for more details.
     */
    epsilon_cutoff?: number;
    /**
     * Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to
     * float strictly between 0 and 1, a token is only considered if it is greater than either
     * eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter
     * term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In
     * the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
     * See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)
     * for more details.
     */
    eta_cutoff?: number;
    /**
     * The maximum length (in tokens) of the generated text, including the input.
     */
    max_length?: number;
    /**
     * The maximum number of tokens to generate. Takes precedence over maxLength.
     */
    max_new_tokens?: number;
    /**
     * The minimum length (in tokens) of the generated text, including the input.
     */
    min_length?: number;
    /**
     * The minimum number of tokens to generate. Takes precedence over maxLength.
     */
    min_new_tokens?: number;
    /**
     * Number of groups to divide num_beams into in order to ensure diversity among different
     * groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.
     */
    num_beam_groups?: number;
    /**
     * Number of beams to use for beam search.
     */
    num_beams?: number;
    /**
     * The value balances the model confidence and the degeneration penalty in contrastive
     * search decoding.
     */
    penalty_alpha?: number;
    /**
     * The value used to modulate the next token probabilities.
     */
    temperature?: number;
    /**
     * The number of highest probability vocabulary tokens to keep for top-k-filtering.
     */
    top_k?: number;
    /**
     * If set to float < 1, only the smallest set of most probable tokens with probabilities
     * that add up to top_p or higher are kept for generation.
     */
    top_p?: number;
    /**
     * Local typicality measures how similar the conditional probability of predicting a target
     * token next is to the expected conditional probability of predicting a random token next,
     * given the partial text already generated. If set to float < 1, the smallest set of the
     * most locally typical tokens with probabilities that add up to typical_p or higher are
     * kept for generation. See [this paper](https://hf.co/papers/2202.00666) for more details.
     */
    typical_p?: number;
    /**
     * Whether the model should use the past last key/values attentions to speed up decoding
     */
    use_cache?: boolean;
    [property: string]: unknown;
}
/**
 * Controls the stopping condition for beam-based methods.
 */
type EarlyStoppingUnion$2 = boolean | "never";
/**
 * Outputs of inference for the Automatic Speech Recognition task
 */
interface AutomaticSpeechRecognitionOutput {
    /**
     * When returnTimestamps is enabled, chunks contains a list of audio chunks identified by
     * the model.
     */
    chunks?: AutomaticSpeechRecognitionOutputChunk[];
    /**
     * The recognized text.
     */
    text: string;
    [property: string]: unknown;
}
interface AutomaticSpeechRecognitionOutputChunk {
    /**
     * A chunk of text identified by the model
     */
    text: string;
    /**
     * The start and end timestamps corresponding with the text
     */
    timestamps: number[];
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for ChatCompletion inference
 */
interface ChatCompletionInput {
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
     * frequency in the text so far, decreasing the model's likelihood to repeat the same line
     * verbatim.
     */
    frequency_penalty?: number;
    /**
     * The maximum number of tokens that can be generated in the chat completion.
     */
    max_tokens?: number;
    messages: ChatCompletionInputMessage[];
    /**
     * The random sampling seed.
     */
    seed?: number;
    /**
     * Stop generating tokens if a stop token is generated.
     */
    stop?: ChatCompletionInputStopReason;
    /**
     * If set, partial message deltas will be sent.
     */
    stream?: boolean;
    /**
     * The value used to modulate the logits distribution.
     */
    temperature?: number;
    /**
     * If set to < 1, only the smallest set of most probable tokens with probabilities that add
     * up to `top_p` or higher are kept for generation.
     */
    top_p?: number;
    [property: string]: unknown;
}
interface ChatCompletionInputMessage {
    /**
     * The content of the message.
     */
    content: string;
    role: ChatCompletionMessageRole;
    [property: string]: unknown;
}
/**
 * The role of the message author.
 */
type ChatCompletionMessageRole = "assistant" | "system" | "user";
/**
 * Stop generating tokens if a stop token is generated.
 */
type ChatCompletionInputStopReason = string[] | string;
/**
 * Outputs for Chat Completion inference
 */
interface ChatCompletionOutput {
    /**
     * A list of chat completion choices.
     */
    choices: ChatCompletionOutputChoice[];
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created.
     */
    created: number;
    [property: string]: unknown;
}
interface ChatCompletionOutputChoice {
    /**
     * The reason why the generation was stopped.
     */
    finish_reason: ChatCompletionFinishReason;
    /**
     * The index of the choice in the list of choices.
     */
    index: number;
    message: ChatCompletionOutputChoiceMessage;
    [property: string]: unknown;
}
/**
 * The reason why the generation was stopped.
 *
 * The generated sequence reached the maximum allowed length
 *
 * The model generated an end-of-sentence (EOS) token
 *
 * One of the sequence in stop_sequences was generated
 */
type ChatCompletionFinishReason = "length" | "eos_token" | "stop_sequence";
interface ChatCompletionOutputChoiceMessage {
    /**
     * The content of the chat completion message.
     */
    content: string;
    role: ChatCompletionMessageRole;
    [property: string]: unknown;
}
/**
 * Chat Completion Stream Output
 */
interface ChatCompletionStreamOutput {
    /**
     * A list of chat completion choices.
     */
    choices: ChatCompletionStreamOutputChoice[];
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has
     * the same timestamp.
     */
    created: number;
    [property: string]: unknown;
}
interface ChatCompletionStreamOutputChoice {
    /**
     * A chat completion delta generated by streamed model responses.
     */
    delta: ChatCompletionStreamOutputDelta;
    /**
     * The reason why the generation was stopped.
     */
    finish_reason?: ChatCompletionFinishReason;
    /**
     * The index of the choice in the list of choices.
     */
    index: number;
    [property: string]: unknown;
}
/**
 * A chat completion delta generated by streamed model responses.
 */
interface ChatCompletionStreamOutputDelta {
    /**
     * The contents of the chunk message.
     */
    content?: string;
    /**
     * The role of the author of this message.
     */
    role?: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Document Question Answering inference
 */
interface DocumentQuestionAnsweringInput {
    /**
     * One (document, question) pair to answer
     */
    inputs: DocumentQuestionAnsweringInputData;
    /**
     * Additional inference parameters
     */
    parameters?: DocumentQuestionAnsweringParameters;
    [property: string]: unknown;
}
/**
 * One (document, question) pair to answer
 */
interface DocumentQuestionAnsweringInputData {
    /**
     * The image on which the question is asked
     */
    image: unknown;
    /**
     * A question to ask of the document
     */
    question: string;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Document Question Answering
 */
interface DocumentQuestionAnsweringParameters {
    /**
     * If the words in the document are too long to fit with the question for the model, it will
     * be split in several chunks with some overlap. This argument controls the size of that
     * overlap.
     */
    doc_stride?: number;
    /**
     * Whether to accept impossible as an answer
     */
    handle_impossible_answer?: boolean;
    /**
     * Language to use while running OCR. Defaults to english.
     */
    lang?: string;
    /**
     * The maximum length of predicted answers (e.g., only answers with a shorter length are
     * considered).
     */
    max_answer_len?: number;
    /**
     * The maximum length of the question after tokenization. It will be truncated if needed.
     */
    max_question_len?: number;
    /**
     * The maximum length of the total sentence (context + question) in tokens of each chunk
     * passed to the model. The context will be split in several chunks (using doc_stride as
     * overlap) if needed.
     */
    max_seq_len?: number;
    /**
     * The number of answers to return (will be chosen by order of likelihood). Can return less
     * than top_k answers if there are not enough options available within the context.
     */
    top_k?: number;
    /**
     * A list of words and bounding boxes (normalized 0->1000). If provided, the inference will
     * skip the OCR step and use the provided bounding boxes instead.
     */
    word_boxes?: WordBox[];
    [property: string]: unknown;
}
type WordBox = number[] | string;
type DocumentQuestionAnsweringOutput = DocumentQuestionAnsweringOutputElement[];
/**
 * Outputs of inference for the Document Question Answering task
 */
interface DocumentQuestionAnsweringOutputElement {
    /**
     * The answer to the question.
     */
    answer: string;
    /**
     * The end word index of the answer (in the OCR’d version of the input or provided word
     * boxes).
     */
    end: number;
    /**
     * The probability associated to the answer.
     */
    score: number;
    /**
     * The start word index of the answer (in the OCR’d version of the input or provided word
     * boxes).
     */
    start: number;
    /**
     * The index of each word/box pair that is in the answer
     */
    words: number[];
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
type FeatureExtractionOutput = unknown[];
/**
 * Inputs for Text Embedding inference
 */
interface FeatureExtractionInput {
    /**
     * The text to get the embeddings of
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: {
        [key: string]: unknown;
    };
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Fill Mask inference
 */
interface FillMaskInput {
    /**
     * The text with masked tokens
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: FillMaskParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Fill Mask
 */
interface FillMaskParameters {
    /**
     * When passed, the model will limit the scores to the passed targets instead of looking up
     * in the whole vocabulary. If the provided targets are not in the model vocab, they will be
     * tokenized and the first resulting token will be used (with a warning, and that might be
     * slower).
     */
    targets?: string[];
    /**
     * When passed, overrides the number of predictions to return.
     */
    top_k?: number;
    [property: string]: unknown;
}
type FillMaskOutput = FillMaskOutputElement[];
/**
 * Outputs of inference for the Fill Mask task
 */
interface FillMaskOutputElement {
    /**
     * The corresponding probability
     */
    score: number;
    /**
     * The corresponding input with the mask token prediction.
     */
    sequence: string;
    /**
     * The predicted token id (to replace the masked one).
     */
    token: number;
    tokenStr: unknown;
    /**
     * The predicted token (to replace the masked one).
     */
    token_str?: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Image Classification inference
 */
interface ImageClassificationInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: ImageClassificationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Image Classification
 */
interface ImageClassificationParameters {
    function_to_apply?: ClassificationOutputTransform$2;
    /**
     * When specified, limits the output to the top K most probable classes.
     */
    top_k?: number;
    [property: string]: unknown;
}
/**
 * The function to apply to the model outputs in order to retrieve the scores.
 */
type ClassificationOutputTransform$2 = "sigmoid" | "softmax" | "none";
type ImageClassificationOutput = ImageClassificationOutputElement[];
/**
 * Outputs of inference for the Image Classification task
 */
interface ImageClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Image To Image inference
 */
interface ImageToImageInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: ImageToImageParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Image To Image
 */
interface ImageToImageParameters {
    /**
     * For diffusion models. A higher guidance scale value encourages the model to generate
     * images closely linked to the text prompt at the expense of lower image quality.
     */
    guidance_scale?: number;
    /**
     * One or several prompt to guide what NOT to include in image generation.
     */
    negative_prompt?: string[];
    /**
     * For diffusion models. The number of denoising steps. More denoising steps usually lead to
     * a higher quality image at the expense of slower inference.
     */
    num_inference_steps?: number;
    /**
     * The size in pixel of the output image
     */
    target_size?: TargetSize$1;
    [property: string]: unknown;
}
/**
 * The size in pixel of the output image
 */
interface TargetSize$1 {
    height: number;
    width: number;
    [property: string]: unknown;
}
/**
 * Outputs of inference for the Image To Image task
 */
interface ImageToImageOutput {
    /**
     * The output image
     */
    image?: unknown;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Image To Text inference
 */
interface ImageToTextInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: ImageToTextParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Image To Text
 */
interface ImageToTextParameters {
    /**
     * Parametrization of the text generation process
     */
    generate?: GenerationParameters$1;
    /**
     * The amount of maximum tokens to generate.
     */
    max_new_tokens?: number;
    [property: string]: unknown;
}
/**
 * Parametrization of the text generation process
 *
 * Ad-hoc parametrization of the text generation process
 */
interface GenerationParameters$1 {
    /**
     * Whether to use sampling instead of greedy decoding when generating new tokens.
     */
    do_sample?: boolean;
    /**
     * Controls the stopping condition for beam-based methods.
     */
    early_stopping?: EarlyStoppingUnion$1;
    /**
     * If set to float strictly between 0 and 1, only tokens with a conditional probability
     * greater than epsilon_cutoff will be sampled. In the paper, suggested values range from
     * 3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language
     * Model Desmoothing](https://hf.co/papers/2210.15191) for more details.
     */
    epsilon_cutoff?: number;
    /**
     * Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to
     * float strictly between 0 and 1, a token is only considered if it is greater than either
     * eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter
     * term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In
     * the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
     * See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)
     * for more details.
     */
    eta_cutoff?: number;
    /**
     * The maximum length (in tokens) of the generated text, including the input.
     */
    max_length?: number;
    /**
     * The maximum number of tokens to generate. Takes precedence over maxLength.
     */
    max_new_tokens?: number;
    /**
     * The minimum length (in tokens) of the generated text, including the input.
     */
    min_length?: number;
    /**
     * The minimum number of tokens to generate. Takes precedence over maxLength.
     */
    min_new_tokens?: number;
    /**
     * Number of groups to divide num_beams into in order to ensure diversity among different
     * groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.
     */
    num_beam_groups?: number;
    /**
     * Number of beams to use for beam search.
     */
    num_beams?: number;
    /**
     * The value balances the model confidence and the degeneration penalty in contrastive
     * search decoding.
     */
    penalty_alpha?: number;
    /**
     * The value used to modulate the next token probabilities.
     */
    temperature?: number;
    /**
     * The number of highest probability vocabulary tokens to keep for top-k-filtering.
     */
    top_k?: number;
    /**
     * If set to float < 1, only the smallest set of most probable tokens with probabilities
     * that add up to top_p or higher are kept for generation.
     */
    top_p?: number;
    /**
     * Local typicality measures how similar the conditional probability of predicting a target
     * token next is to the expected conditional probability of predicting a random token next,
     * given the partial text already generated. If set to float < 1, the smallest set of the
     * most locally typical tokens with probabilities that add up to typical_p or higher are
     * kept for generation. See [this paper](https://hf.co/papers/2202.00666) for more details.
     */
    typical_p?: number;
    /**
     * Whether the model should use the past last key/values attentions to speed up decoding
     */
    use_cache?: boolean;
    [property: string]: unknown;
}
/**
 * Controls the stopping condition for beam-based methods.
 */
type EarlyStoppingUnion$1 = boolean | "never";
/**
 * Outputs of inference for the Image To Text task
 */
interface ImageToTextOutput {
    generatedText: unknown;
    /**
     * The generated text.
     */
    generated_text?: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Image Segmentation inference
 */
interface ImageSegmentationInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: ImageSegmentationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Image Segmentation
 */
interface ImageSegmentationParameters {
    /**
     * Threshold to use when turning the predicted masks into binary values.
     */
    mask_threshold?: number;
    /**
     * Mask overlap threshold to eliminate small, disconnected segments.
     */
    overlap_mask_area_threshold?: number;
    /**
     * Segmentation task to be performed, depending on model capabilities.
     */
    subtask?: ImageSegmentationSubtask;
    /**
     * Probability threshold to filter out predicted masks.
     */
    threshold?: number;
    [property: string]: unknown;
}
type ImageSegmentationSubtask = "instance" | "panoptic" | "semantic";
type ImageSegmentationOutput = ImageSegmentationOutputElement[];
/**
 * Outputs of inference for the Image Segmentation task
 *
 * A predicted mask / segment
 */
interface ImageSegmentationOutputElement {
    /**
     * The label of the predicted segment
     */
    label: string;
    /**
     * The corresponding mask as a black-and-white image
     */
    mask: unknown;
    /**
     * The score or confidence degreee the model has
     */
    score?: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Object Detection inference
 */
interface ObjectDetectionInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: ObjectDetectionParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Object Detection
 */
interface ObjectDetectionParameters {
    /**
     * The probability necessary to make a prediction.
     */
    threshold?: number;
    [property: string]: unknown;
}
/**
 * The predicted bounding box. Coordinates are relative to the top left corner of the input
 * image.
 */
interface BoundingBox$1 {
    xmax: number;
    xmin: number;
    ymax: number;
    ymin: number;
    [property: string]: unknown;
}
type ObjectDetectionOutput = ObjectDetectionOutputElement[];
/**
 * Outputs of inference for the Object Detection task
 */
interface ObjectDetectionOutputElement {
    /**
     * The predicted bounding box. Coordinates are relative to the top left corner of the input
     * image.
     */
    box: BoundingBox$1;
    /**
     * The predicted label for the bounding box
     */
    label: string;
    /**
     * The associated score / probability
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Depth Estimation inference
 */
interface DepthEstimationInput {
    /**
     * The input image data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: {
        [key: string]: unknown;
    };
    [property: string]: unknown;
}
/**
 * Outputs of inference for the Depth Estimation task
 */
interface DepthEstimationOutput {
    /**
     * The predicted depth as an image
     */
    depth?: unknown;
    /**
     * The predicted depth as a tensor
     */
    predicted_depth?: unknown;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Question Answering inference
 */
interface QuestionAnsweringInput {
    /**
     * One (context, question) pair to answer
     */
    inputs: QuestionAnsweringInputData;
    /**
     * Additional inference parameters
     */
    parameters?: QuestionAnsweringParameters;
    [property: string]: unknown;
}
/**
 * One (context, question) pair to answer
 */
interface QuestionAnsweringInputData {
    /**
     * The context to be used for answering the question
     */
    context: string;
    /**
     * The question to be answered
     */
    question: string;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Question Answering
 */
interface QuestionAnsweringParameters {
    /**
     * Attempts to align the answer to real words. Improves quality on space separated
     * languages. Might hurt on non-space-separated languages (like Japanese or Chinese)
     */
    align_to_words?: boolean;
    /**
     * If the context is too long to fit with the question for the model, it will be split in
     * several chunks with some overlap. This argument controls the size of that overlap.
     */
    doc_stride?: number;
    /**
     * Whether to accept impossible as an answer.
     */
    handle_impossible_answer?: boolean;
    /**
     * The maximum length of predicted answers (e.g., only answers with a shorter length are
     * considered).
     */
    max_answer_len?: number;
    /**
     * The maximum length of the question after tokenization. It will be truncated if needed.
     */
    max_question_len?: number;
    /**
     * The maximum length of the total sentence (context + question) in tokens of each chunk
     * passed to the model. The context will be split in several chunks (using docStride as
     * overlap) if needed.
     */
    max_seq_len?: number;
    /**
     * The number of answers to return (will be chosen by order of likelihood). Note that we
     * return less than topk answers if there are not enough options available within the
     * context.
     */
    top_k?: number;
    [property: string]: unknown;
}
type QuestionAnsweringOutput = QuestionAnsweringOutputElement[];
/**
 * Outputs of inference for the Question Answering task
 */
interface QuestionAnsweringOutputElement {
    /**
     * The answer to the question.
     */
    answer: string;
    /**
     * The character position in the input where the answer ends.
     */
    end: number;
    /**
     * The probability associated to the answer.
     */
    score: number;
    /**
     * The character position in the input where the answer begins.
     */
    start: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
type SentenceSimilarityOutput = number[];
/**
 * Inputs for Sentence similarity inference
 */
interface SentenceSimilarityInput {
    inputs: SentenceSimilarityInputData;
    /**
     * Additional inference parameters
     */
    parameters?: {
        [key: string]: unknown;
    };
    [property: string]: unknown;
}
interface SentenceSimilarityInputData {
    /**
     * A list of strings which will be compared against the source_sentence.
     */
    sentences: string[];
    /**
     * The string that you wish to compare the other strings with. This can be a phrase,
     * sentence, or longer passage, depending on the model being used.
     */
    sourceSentence: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Summarization inference
 *
 * Inputs for Text2text Generation inference
 */
interface SummarizationInput {
    /**
     * The input text data
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: Text2TextGenerationParameters$1;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text2text Generation
 */
interface Text2TextGenerationParameters$1 {
    /**
     * Whether to clean up the potential extra spaces in the text output.
     */
    clean_up_tokenization_spaces?: boolean;
    /**
     * Additional parametrization of the text generation algorithm
     */
    generate_parameters?: {
        [key: string]: unknown;
    };
    /**
     * The truncation strategy to use
     */
    truncation?: Text2TextGenerationTruncationStrategy$1;
    [property: string]: unknown;
}
type Text2TextGenerationTruncationStrategy$1 = "do_not_truncate" | "longest_first" | "only_first" | "only_second";
/**
 * Outputs of inference for the Summarization task
 */
interface SummarizationOutput {
    /**
     * The summarized text.
     */
    summary_text: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Table Question Answering inference
 */
interface TableQuestionAnsweringInput {
    /**
     * One (table, question) pair to answer
     */
    inputs: TableQuestionAnsweringInputData;
    /**
     * Additional inference parameters
     */
    parameters?: {
        [key: string]: unknown;
    };
    [property: string]: unknown;
}
/**
 * One (table, question) pair to answer
 */
interface TableQuestionAnsweringInputData {
    /**
     * The question to be answered about the table
     */
    question: string;
    /**
     * The table to serve as context for the questions
     */
    table: {
        [key: string]: string[];
    };
    [property: string]: unknown;
}
type TableQuestionAnsweringOutput = TableQuestionAnsweringOutputElement[];
/**
 * Outputs of inference for the Table Question Answering task
 */
interface TableQuestionAnsweringOutputElement {
    /**
     * If the model has an aggregator, this returns the aggregator.
     */
    aggregator?: string;
    /**
     * The answer of the question given the table. If there is an aggregator, the answer will be
     * preceded by `AGGREGATOR >`.
     */
    answer: string;
    /**
     * List of strings made up of the answer cell values.
     */
    cells: string[];
    /**
     * Coordinates of the cells of the answers.
     */
    coordinates: Array<number[]>;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Text To Image inference
 */
interface TextToImageInput {
    /**
     * The input text data (sometimes called "prompt"
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: TextToImageParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text To Image
 */
interface TextToImageParameters {
    /**
     * For diffusion models. A higher guidance scale value encourages the model to generate
     * images closely linked to the text prompt at the expense of lower image quality.
     */
    guidance_scale?: number;
    /**
     * One or several prompt to guide what NOT to include in image generation.
     */
    negative_prompt?: string[];
    /**
     * For diffusion models. The number of denoising steps. More denoising steps usually lead to
     * a higher quality image at the expense of slower inference.
     */
    num_inference_steps?: number;
    /**
     * For diffusion models. Override the scheduler with a compatible one
     */
    scheduler?: string;
    /**
     * The size in pixel of the output image
     */
    target_size?: TargetSize;
    [property: string]: unknown;
}
/**
 * The size in pixel of the output image
 */
interface TargetSize {
    height: number;
    width: number;
    [property: string]: unknown;
}
/**
 * Outputs of inference for the Text To Image task
 */
interface TextToImageOutput {
    /**
     * The generated image
     */
    image: unknown;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Text to Speech inference
 *
 * Inputs for Text To Audio inference
 */
interface TextToSpeechInput {
    /**
     * The input text data
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: TextToAudioParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text To Audio
 */
interface TextToAudioParameters {
    /**
     * Parametrization of the text generation process
     */
    generate?: GenerationParameters;
    [property: string]: unknown;
}
/**
 * Parametrization of the text generation process
 *
 * Ad-hoc parametrization of the text generation process
 */
interface GenerationParameters {
    /**
     * Whether to use sampling instead of greedy decoding when generating new tokens.
     */
    do_sample?: boolean;
    /**
     * Controls the stopping condition for beam-based methods.
     */
    early_stopping?: EarlyStoppingUnion;
    /**
     * If set to float strictly between 0 and 1, only tokens with a conditional probability
     * greater than epsilon_cutoff will be sampled. In the paper, suggested values range from
     * 3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language
     * Model Desmoothing](https://hf.co/papers/2210.15191) for more details.
     */
    epsilon_cutoff?: number;
    /**
     * Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to
     * float strictly between 0 and 1, a token is only considered if it is greater than either
     * eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter
     * term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In
     * the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
     * See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)
     * for more details.
     */
    eta_cutoff?: number;
    /**
     * The maximum length (in tokens) of the generated text, including the input.
     */
    max_length?: number;
    /**
     * The maximum number of tokens to generate. Takes precedence over maxLength.
     */
    max_new_tokens?: number;
    /**
     * The minimum length (in tokens) of the generated text, including the input.
     */
    min_length?: number;
    /**
     * The minimum number of tokens to generate. Takes precedence over maxLength.
     */
    min_new_tokens?: number;
    /**
     * Number of groups to divide num_beams into in order to ensure diversity among different
     * groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.
     */
    num_beam_groups?: number;
    /**
     * Number of beams to use for beam search.
     */
    num_beams?: number;
    /**
     * The value balances the model confidence and the degeneration penalty in contrastive
     * search decoding.
     */
    penalty_alpha?: number;
    /**
     * The value used to modulate the next token probabilities.
     */
    temperature?: number;
    /**
     * The number of highest probability vocabulary tokens to keep for top-k-filtering.
     */
    top_k?: number;
    /**
     * If set to float < 1, only the smallest set of most probable tokens with probabilities
     * that add up to top_p or higher are kept for generation.
     */
    top_p?: number;
    /**
     * Local typicality measures how similar the conditional probability of predicting a target
     * token next is to the expected conditional probability of predicting a random token next,
     * given the partial text already generated. If set to float < 1, the smallest set of the
     * most locally typical tokens with probabilities that add up to typical_p or higher are
     * kept for generation. See [this paper](https://hf.co/papers/2202.00666) for more details.
     */
    typical_p?: number;
    /**
     * Whether the model should use the past last key/values attentions to speed up decoding
     */
    use_cache?: boolean;
    [property: string]: unknown;
}
/**
 * Controls the stopping condition for beam-based methods.
 */
type EarlyStoppingUnion = boolean | "never";
/**
 * Outputs for Text to Speech inference
 *
 * Outputs of inference for the Text To Audio task
 */
interface TextToSpeechOutput {
    /**
     * The generated audio waveform.
     */
    audio: unknown;
    samplingRate: unknown;
    /**
     * The sampling rate of the generated audio waveform.
     */
    sampling_rate?: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Token Classification inference
 */
interface TokenClassificationInput {
    /**
     * The input text data
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: TokenClassificationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Token Classification
 */
interface TokenClassificationParameters {
    /**
     * The strategy used to fuse tokens based on model predictions
     */
    aggregation_strategy?: TokenClassificationAggregationStrategy;
    /**
     * A list of labels to ignore
     */
    ignore_labels?: string[];
    /**
     * The number of overlapping tokens between chunks when splitting the input text.
     */
    stride?: number;
    [property: string]: unknown;
}
/**
 * Do not aggregate tokens
 *
 * Group consecutive tokens with the same label in a single entity.
 *
 * Similar to "simple", also preserves word integrity (use the label predicted for the first
 * token in a word).
 *
 * Similar to "simple", also preserves word integrity (uses the label with the highest
 * score, averaged across the word's tokens).
 *
 * Similar to "simple", also preserves word integrity (uses the label with the highest score
 * across the word's tokens).
 */
type TokenClassificationAggregationStrategy = "none" | "simple" | "first" | "average" | "max";
type TokenClassificationOutput = TokenClassificationOutputElement[];
/**
 * Outputs of inference for the Token Classification task
 */
interface TokenClassificationOutputElement {
    /**
     * The character position in the input where this group ends.
     */
    end?: number;
    /**
     * The predicted label for that group of tokens
     */
    entity_group?: string;
    label: unknown;
    /**
     * The associated score / probability
     */
    score: number;
    /**
     * The character position in the input where this group begins.
     */
    start?: number;
    /**
     * The corresponding text
     */
    word?: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Translation inference
 *
 * Inputs for Text2text Generation inference
 */
interface TranslationInput {
    /**
     * The input text data
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: Text2TextGenerationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text2text Generation
 */
interface Text2TextGenerationParameters {
    /**
     * Whether to clean up the potential extra spaces in the text output.
     */
    clean_up_tokenization_spaces?: boolean;
    /**
     * Additional parametrization of the text generation algorithm
     */
    generate_parameters?: {
        [key: string]: unknown;
    };
    /**
     * The truncation strategy to use
     */
    truncation?: Text2TextGenerationTruncationStrategy;
    [property: string]: unknown;
}
type Text2TextGenerationTruncationStrategy = "do_not_truncate" | "longest_first" | "only_first" | "only_second";
/**
 * Outputs of inference for the Translation task
 */
interface TranslationOutput {
    /**
     * The translated text.
     */
    translation_text: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Text Classification inference
 */
interface TextClassificationInput {
    /**
     * The text to classify
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: TextClassificationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text Classification
 */
interface TextClassificationParameters {
    function_to_apply?: ClassificationOutputTransform$1;
    /**
     * When specified, limits the output to the top K most probable classes.
     */
    top_k?: number;
    [property: string]: unknown;
}
/**
 * The function to apply to the model outputs in order to retrieve the scores.
 */
type ClassificationOutputTransform$1 = "sigmoid" | "softmax" | "none";
type TextClassificationOutput = TextClassificationOutputElement[];
/**
 * Outputs of inference for the Text Classification task
 */
interface TextClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Text Generation inference
 */
interface TextGenerationInput {
    /**
     * The text to initialize generation with
     */
    inputs: string;
    /**
     * Additional inference parameters
     */
    parameters?: TextGenerationParameters;
    /**
     * Whether to stream output tokens
     */
    stream?: boolean;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Text Generation
 */
interface TextGenerationParameters {
    /**
     * The number of sampling queries to run. Only the best one (in terms of total logprob) will
     * be returned.
     */
    best_of?: number;
    /**
     * Whether or not to output decoder input details
     */
    decoder_input_details?: boolean;
    /**
     * Whether or not to output details
     */
    details?: boolean;
    /**
     * Whether to use logits sampling instead of greedy decoding when generating new tokens.
     */
    do_sample?: boolean;
    /**
     * The maximum number of tokens to generate.
     */
    max_new_tokens?: number;
    /**
     * The parameter for repetition penalty. A value of 1.0 means no penalty. See [this
     * paper](https://hf.co/papers/1909.05858) for more details.
     */
    repetition_penalty?: number;
    /**
     * Whether to prepend the prompt to the generated text.
     */
    return_full_text?: boolean;
    /**
     * The random sampling seed.
     */
    seed?: number;
    /**
     * Stop generating tokens if a member of `stop_sequences` is generated.
     */
    stop_sequences?: string[];
    /**
     * The value used to modulate the logits distribution.
     */
    temperature?: number;
    /**
     * The number of highest probability vocabulary tokens to keep for top-k-filtering.
     */
    top_k?: number;
    /**
     * If set to < 1, only the smallest set of most probable tokens with probabilities that add
     * up to `top_p` or higher are kept for generation.
     */
    top_p?: number;
    /**
     * Truncate input tokens to the given size.
     */
    truncate?: number;
    /**
     * Typical Decoding mass. See [Typical Decoding for Natural Language
     * Generation](https://hf.co/papers/2202.00666) for more information
     */
    typical_p?: number;
    /**
     * Watermarking with [A Watermark for Large Language Models](https://hf.co/papers/2301.10226)
     */
    watermark?: boolean;
    [property: string]: unknown;
}
/**
 * Outputs for Text Generation inference
 */
interface TextGenerationOutput {
    /**
     * When enabled, details about the generation
     */
    details?: TextGenerationOutputDetails;
    /**
     * The generated text
     */
    generated_text: string;
    [property: string]: unknown;
}
/**
 * When enabled, details about the generation
 */
interface TextGenerationOutputDetails {
    /**
     * Details about additional sequences when best_of is provided
     */
    best_of_sequences?: TextGenerationOutputSequenceDetails[];
    /**
     * The reason why the generation was stopped.
     */
    finish_reason: TextGenerationFinishReason;
    /**
     * The number of generated tokens
     */
    generated_tokens: number;
    prefill: TextGenerationPrefillToken[];
    /**
     * The random seed used for generation
     */
    seed?: number;
    /**
     * The generated tokens and associated details
     */
    tokens: TextGenerationOutputToken[];
    /**
     * Most likely tokens
     */
    top_tokens?: Array<TextGenerationOutputToken[]>;
    [property: string]: unknown;
}
interface TextGenerationOutputSequenceDetails {
    finish_reason: TextGenerationFinishReason;
    /**
     * The generated text
     */
    generated_text: string;
    /**
     * The number of generated tokens
     */
    generated_tokens: number;
    prefill: TextGenerationPrefillToken[];
    /**
     * The random seed used for generation
     */
    seed?: number;
    /**
     * The generated tokens and associated details
     */
    tokens: TextGenerationOutputToken[];
    /**
     * Most likely tokens
     */
    top_tokens?: Array<TextGenerationOutputToken[]>;
    [property: string]: unknown;
}
/**
 * The reason why the generation was stopped.
 *
 * length: The generated sequence reached the maximum allowed length
 *
 * eos_token: The model generated an end-of-sentence (EOS) token
 *
 * stop_sequence: One of the sequence in stop_sequences was generated
 */
type TextGenerationFinishReason = "length" | "eos_token" | "stop_sequence";
interface TextGenerationPrefillToken {
    id: number;
    logprob: number;
    /**
     * The text associated with that token
     */
    text: string;
    [property: string]: unknown;
}
/**
 * Generated token.
 */
interface TextGenerationOutputToken {
    id: number;
    logprob?: number;
    /**
     * Whether or not that token is a special one
     */
    special: boolean;
    /**
     * The text associated with that token
     */
    text: string;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Video Classification inference
 */
interface VideoClassificationInput {
    /**
     * The input video data
     */
    inputs: unknown;
    /**
     * Additional inference parameters
     */
    parameters?: VideoClassificationParameters;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Video Classification
 */
interface VideoClassificationParameters {
    /**
     * The sampling rate used to select frames from the video.
     */
    frame_sampling_rate?: number;
    function_to_apply?: ClassificationOutputTransform;
    /**
     * The number of sampled frames to consider for classification.
     */
    num_frames?: number;
    /**
     * When specified, limits the output to the top K most probable classes.
     */
    top_k?: number;
    [property: string]: unknown;
}
/**
 * The function to apply to the model outputs in order to retrieve the scores.
 */
type ClassificationOutputTransform = "sigmoid" | "softmax" | "none";
type VideoClassificationOutput = VideoClassificationOutputElement[];
/**
 * Outputs of inference for the Video Classification task
 */
interface VideoClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Visual Question Answering inference
 */
interface VisualQuestionAnsweringInput {
    /**
     * One (image, question) pair to answer
     */
    inputs: VisualQuestionAnsweringInputData;
    /**
     * Additional inference parameters
     */
    parameters?: VisualQuestionAnsweringParameters;
    [property: string]: unknown;
}
/**
 * One (image, question) pair to answer
 */
interface VisualQuestionAnsweringInputData {
    /**
     * The image.
     */
    image: unknown;
    /**
     * The question to answer based on the image.
     */
    question: unknown;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Visual Question Answering
 */
interface VisualQuestionAnsweringParameters {
    /**
     * The number of answers to return (will be chosen by order of likelihood). Note that we
     * return less than topk answers if there are not enough options available within the
     * context.
     */
    top_k?: number;
    [property: string]: unknown;
}
type VisualQuestionAnsweringOutput = VisualQuestionAnsweringOutputElement[];
/**
 * Outputs of inference for the Visual Question Answering task
 */
interface VisualQuestionAnsweringOutputElement {
    /**
     * The answer to the question
     */
    answer?: string;
    label: unknown;
    /**
     * The associated score / probability
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Zero Shot Classification inference
 */
interface ZeroShotClassificationInput {
    /**
     * The input text data, with candidate labels
     */
    inputs: ZeroShotClassificationInputData;
    /**
     * Additional inference parameters
     */
    parameters?: ZeroShotClassificationParameters;
    [property: string]: unknown;
}
/**
 * The input text data, with candidate labels
 */
interface ZeroShotClassificationInputData {
    /**
     * The set of possible class labels to classify the text into.
     */
    candidateLabels: string[];
    /**
     * The text to classify
     */
    text: string;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Zero Shot Classification
 */
interface ZeroShotClassificationParameters {
    /**
     * The sentence used in conjunction with candidateLabels to attempt the text classification
     * by replacing the placeholder with the candidate labels.
     */
    hypothesis_template?: string;
    /**
     * Whether multiple candidate labels can be true. If false, the scores are normalized such
     * that the sum of the label likelihoods for each sequence is 1. If true, the labels are
     * considered independent and probabilities are normalized for each candidate.
     */
    multi_label?: boolean;
    [property: string]: unknown;
}
type ZeroShotClassificationOutput = ZeroShotClassificationOutputElement[];
/**
 * Outputs of inference for the Zero Shot Classification task
 */
interface ZeroShotClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Zero Shot Image Classification inference
 */
interface ZeroShotImageClassificationInput {
    /**
     * The input image data, with candidate labels
     */
    inputs: ZeroShotImageClassificationInputData;
    /**
     * Additional inference parameters
     */
    parameters?: ZeroShotImageClassificationParameters;
    [property: string]: unknown;
}
/**
 * The input image data, with candidate labels
 */
interface ZeroShotImageClassificationInputData {
    /**
     * The candidate labels for this image
     */
    candidateLabels: string[];
    /**
     * The image data to classify
     */
    image: unknown;
    [property: string]: unknown;
}
/**
 * Additional inference parameters
 *
 * Additional inference parameters for Zero Shot Image Classification
 */
interface ZeroShotImageClassificationParameters {
    /**
     * The sentence used in conjunction with candidateLabels to attempt the text classification
     * by replacing the placeholder with the candidate labels.
     */
    hypothesis_template?: string;
    [property: string]: unknown;
}
type ZeroShotImageClassificationOutput = ZeroShotImageClassificationOutputElement[];
/**
 * Outputs of inference for the Zero Shot Image Classification task
 */
interface ZeroShotImageClassificationOutputElement {
    /**
     * The predicted class label.
     */
    label: string;
    /**
     * The corresponding probability.
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Inputs for Zero Shot Object Detection inference
 */
interface ZeroShotObjectDetectionInput {
    /**
     * The input image data, with candidate labels
     */
    inputs: ZeroShotObjectDetectionInputData;
    /**
     * Additional inference parameters
     */
    parameters?: {
        [key: string]: unknown;
    };
    [property: string]: unknown;
}
/**
 * The input image data, with candidate labels
 */
interface ZeroShotObjectDetectionInputData {
    /**
     * The candidate labels for this image
     */
    candidateLabels: string[];
    /**
     * The image data to generate bounding boxes from
     */
    image: unknown;
    [property: string]: unknown;
}
/**
 * The predicted bounding box. Coordinates are relative to the top left corner of the input
 * image.
 */
interface BoundingBox {
    xmax: number;
    xmin: number;
    ymax: number;
    ymin: number;
    [property: string]: unknown;
}
type ZeroShotObjectDetectionOutput = ZeroShotObjectDetectionOutputElement[];
/**
 * Outputs of inference for the Zero Shot Object Detection task
 */
interface ZeroShotObjectDetectionOutputElement {
    /**
     * The predicted bounding box. Coordinates are relative to the top left corner of the input
     * image.
     */
    box: BoundingBox;
    /**
     * A candidate label
     */
    label: string;
    /**
     * The associated score / probability
     */
    score: number;
    [property: string]: unknown;
}

/**
 * Model libraries compatible with each ML task
 */
declare const TASKS_MODEL_LIBRARIES: Record<PipelineType, ModelLibraryKey[]>;
declare const TASKS_DATA: Record<PipelineType, TaskData | undefined>;
interface ExampleRepo {
    description: string;
    id: string;
}
type TaskDemoEntry = {
    filename: string;
    type: "audio";
} | {
    data: Array<{
        label: string;
        score: number;
    }>;
    type: "chart";
} | {
    filename: string;
    type: "img";
} | {
    table: string[][];
    type: "tabular";
} | {
    content: string;
    label: string;
    type: "text";
} | {
    text: string;
    tokens: Array<{
        end: number;
        start: number;
        type: string;
    }>;
    type: "text-with-tokens";
};
interface TaskDemo {
    inputs: TaskDemoEntry[];
    outputs: TaskDemoEntry[];
}
interface TaskData {
    datasets: ExampleRepo[];
    demo: TaskDemo;
    id: PipelineType;
    canonicalId?: PipelineType;
    isPlaceholder?: boolean;
    label: string;
    libraries: ModelLibraryKey[];
    metrics: ExampleRepo[];
    models: ExampleRepo[];
    spaces: ExampleRepo[];
    summary: string;
    widgetModels: string[];
    youtubeId?: string;
}
type TaskDataCustom = Omit<TaskData, "id" | "label" | "libraries">;

/**
 * Minimal model data required for snippets.
 *
 * Add more fields as needed.
 */
type ModelDataMinimal = Pick<ModelData, "id" | "pipeline_tag" | "mask_token" | "library_name">;

declare function getModelInputSnippet(model: ModelDataMinimal, noWrap?: boolean, noQuotes?: boolean): string;

declare const inputs_getModelInputSnippet: typeof getModelInputSnippet;
declare namespace inputs {
  export {
    inputs_getModelInputSnippet as getModelInputSnippet,
  };
}

declare const snippetBasic$2: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetZeroShotClassification$2: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetFile$2: (model: ModelDataMinimal, accessToken: string) => string;
declare const curlSnippets: Partial<Record<PipelineType, (model: ModelDataMinimal, accessToken: string) => string>>;
declare function getCurlInferenceSnippet(model: ModelDataMinimal, accessToken: string): string;
declare function hasCurlInferenceSnippet(model: Pick<ModelDataMinimal, "pipeline_tag">): boolean;

declare const curl_curlSnippets: typeof curlSnippets;
declare const curl_getCurlInferenceSnippet: typeof getCurlInferenceSnippet;
declare const curl_hasCurlInferenceSnippet: typeof hasCurlInferenceSnippet;
declare namespace curl {
  export {
    curl_curlSnippets as curlSnippets,
    curl_getCurlInferenceSnippet as getCurlInferenceSnippet,
    curl_hasCurlInferenceSnippet as hasCurlInferenceSnippet,
    snippetBasic$2 as snippetBasic,
    snippetFile$2 as snippetFile,
    snippetZeroShotClassification$2 as snippetZeroShotClassification,
  };
}

declare const snippetZeroShotClassification$1: (model: ModelDataMinimal) => string;
declare const snippetZeroShotImageClassification: (model: ModelDataMinimal) => string;
declare const snippetBasic$1: (model: ModelDataMinimal) => string;
declare const snippetFile$1: (model: ModelDataMinimal) => string;
declare const snippetTextToImage$1: (model: ModelDataMinimal) => string;
declare const snippetTabular: (model: ModelDataMinimal) => string;
declare const snippetTextToAudio$1: (model: ModelDataMinimal) => string;
declare const snippetDocumentQuestionAnswering: (model: ModelDataMinimal) => string;
declare const pythonSnippets: Partial<Record<PipelineType, (model: ModelDataMinimal) => string>>;
declare function getPythonInferenceSnippet(model: ModelDataMinimal, accessToken: string): string;
declare function hasPythonInferenceSnippet(model: ModelDataMinimal): boolean;

declare const python_getPythonInferenceSnippet: typeof getPythonInferenceSnippet;
declare const python_hasPythonInferenceSnippet: typeof hasPythonInferenceSnippet;
declare const python_pythonSnippets: typeof pythonSnippets;
declare const python_snippetDocumentQuestionAnswering: typeof snippetDocumentQuestionAnswering;
declare const python_snippetTabular: typeof snippetTabular;
declare const python_snippetZeroShotImageClassification: typeof snippetZeroShotImageClassification;
declare namespace python {
  export {
    python_getPythonInferenceSnippet as getPythonInferenceSnippet,
    python_hasPythonInferenceSnippet as hasPythonInferenceSnippet,
    python_pythonSnippets as pythonSnippets,
    snippetBasic$1 as snippetBasic,
    python_snippetDocumentQuestionAnswering as snippetDocumentQuestionAnswering,
    snippetFile$1 as snippetFile,
    python_snippetTabular as snippetTabular,
    snippetTextToAudio$1 as snippetTextToAudio,
    snippetTextToImage$1 as snippetTextToImage,
    snippetZeroShotClassification$1 as snippetZeroShotClassification,
    python_snippetZeroShotImageClassification as snippetZeroShotImageClassification,
  };
}

declare const snippetBasic: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetZeroShotClassification: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetTextToImage: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetTextToAudio: (model: ModelDataMinimal, accessToken: string) => string;
declare const snippetFile: (model: ModelDataMinimal, accessToken: string) => string;
declare const jsSnippets: Partial<Record<PipelineType, (model: ModelDataMinimal, accessToken: string) => string>>;
declare function getJsInferenceSnippet(model: ModelDataMinimal, accessToken: string): string;
declare function hasJsInferenceSnippet(model: ModelDataMinimal): boolean;

declare const js_getJsInferenceSnippet: typeof getJsInferenceSnippet;
declare const js_hasJsInferenceSnippet: typeof hasJsInferenceSnippet;
declare const js_jsSnippets: typeof jsSnippets;
declare const js_snippetBasic: typeof snippetBasic;
declare const js_snippetFile: typeof snippetFile;
declare const js_snippetTextToAudio: typeof snippetTextToAudio;
declare const js_snippetTextToImage: typeof snippetTextToImage;
declare const js_snippetZeroShotClassification: typeof snippetZeroShotClassification;
declare namespace js {
  export {
    js_getJsInferenceSnippet as getJsInferenceSnippet,
    js_hasJsInferenceSnippet as hasJsInferenceSnippet,
    js_jsSnippets as jsSnippets,
    js_snippetBasic as snippetBasic,
    js_snippetFile as snippetFile,
    js_snippetTextToAudio as snippetTextToAudio,
    js_snippetTextToImage as snippetTextToImage,
    js_snippetZeroShotClassification as snippetZeroShotClassification,
  };
}

declare const index_curl: typeof curl;
declare const index_inputs: typeof inputs;
declare const index_js: typeof js;
declare const index_python: typeof python;
declare namespace index {
  export {
    index_curl as curl,
    index_inputs as inputs,
    index_js as js,
    index_python as python,
  };
}

export { ALL_DISPLAY_MODEL_LIBRARY_KEYS, ALL_MODEL_LIBRARY_KEYS, AddedToken, AudioClassificationInput, AudioClassificationOutput, AudioClassificationOutputElement, AudioClassificationParameters, AutomaticSpeechRecognitionInput, AutomaticSpeechRecognitionOutput, AutomaticSpeechRecognitionOutputChunk, AutomaticSpeechRecognitionParameters, BoundingBox, ChatCompletionFinishReason, ChatCompletionInput, ChatCompletionInputMessage, ChatCompletionOutput, ChatCompletionOutputChoice, ChatCompletionOutputChoiceMessage, ChatCompletionStreamOutput, ChatCompletionStreamOutputChoice, ChatCompletionStreamOutputDelta, ChatMessage, ClassificationOutputTransform$1 as ClassificationOutputTransform, DepthEstimationInput, DepthEstimationOutput, DocumentQuestionAnsweringInput, DocumentQuestionAnsweringInputData, DocumentQuestionAnsweringOutput, DocumentQuestionAnsweringOutputElement, DocumentQuestionAnsweringParameters, EarlyStoppingUnion$2 as EarlyStoppingUnion, ExampleRepo, FeatureExtractionInput, FeatureExtractionOutput, FillMaskInput, FillMaskOutput, FillMaskOutputElement, FillMaskParameters, GenerationParameters$2 as GenerationParameters, ImageClassificationInput, ImageClassificationOutput, ImageClassificationOutputElement, ImageClassificationParameters, ImageSegmentationInput, ImageSegmentationOutput, ImageSegmentationOutputElement, ImageSegmentationParameters, ImageSegmentationSubtask, ImageToImageInput, ImageToImageOutput, ImageToImageParameters, ImageToTextInput, ImageToTextOutput, ImageToTextParameters, InferenceDisplayability, LIBRARY_TASK_MAPPING_EXCLUDING_TRANSFORMERS, LibraryUiElement, MAPPING_DEFAULT_WIDGET, MODALITIES, MODALITY_LABELS, MODEL_LIBRARIES_UI_ELEMENTS, Modality, ModelData, ModelLibraryKey, ObjectDetectionInput, ObjectDetectionOutput, ObjectDetectionOutputElement, ObjectDetectionParameters, PIPELINE_DATA, PIPELINE_TYPES, PIPELINE_TYPES_SET, PipelineData, PipelineType, QuestionAnsweringInput, QuestionAnsweringInputData, QuestionAnsweringOutput, QuestionAnsweringOutputElement, QuestionAnsweringParameters, SPECIAL_TOKENS_ATTRIBUTES, SUBTASK_TYPES, SentenceSimilarityInput, SentenceSimilarityInputData, SentenceSimilarityOutput, SpecialTokensMap, SummarizationInput, SummarizationOutput, TASKS_DATA, TASKS_MODEL_LIBRARIES, TableQuestionAnsweringInput, TableQuestionAnsweringInputData, TableQuestionAnsweringOutput, TableQuestionAnsweringOutputElement, TargetSize$1 as TargetSize, TaskData, TaskDataCustom, TaskDemo, TaskDemoEntry, Text2TextGenerationParameters, Text2TextGenerationTruncationStrategy, TextClassificationInput, TextClassificationOutput, TextClassificationOutputElement, TextClassificationParameters, TextGenerationFinishReason, TextGenerationInput, TextGenerationOutput, TextGenerationOutputDetails, TextGenerationOutputSequenceDetails, TextGenerationOutputToken, TextGenerationParameters, TextGenerationPrefillToken, TextToAudioParameters, TextToImageInput, TextToImageOutput, TextToImageParameters, TextToSpeechInput, TextToSpeechOutput, TokenClassificationAggregationStrategy, TokenClassificationInput, TokenClassificationOutput, TokenClassificationOutputElement, TokenClassificationParameters, TokenizerConfig, TransformersInfo, TranslationInput, TranslationOutput, VideoClassificationInput, VideoClassificationOutput, VideoClassificationOutputElement, VideoClassificationParameters, VisualQuestionAnsweringInput, VisualQuestionAnsweringInputData, VisualQuestionAnsweringOutput, VisualQuestionAnsweringOutputElement, VisualQuestionAnsweringParameters, WidgetExample, WidgetExampleAssetAndPromptInput, WidgetExampleAssetAndTextInput, WidgetExampleAssetAndZeroShotInput, WidgetExampleAssetInput, WidgetExampleAttribute, WidgetExampleChatInput, WidgetExampleOutput, WidgetExampleOutputAnswerScore, WidgetExampleOutputLabels, WidgetExampleOutputText, WidgetExampleOutputUrl, WidgetExampleSentenceSimilarityInput, WidgetExampleStructuredDataInput, WidgetExampleTableDataInput, WidgetExampleTextAndContextInput, WidgetExampleTextAndTableInput, WidgetExampleTextInput, WidgetExampleZeroShotTextInput, WidgetType, WordBox, ZeroShotClassificationInput, ZeroShotClassificationInputData, ZeroShotClassificationOutput, ZeroShotClassificationOutputElement, ZeroShotClassificationParameters, ZeroShotImageClassificationInput, ZeroShotImageClassificationInputData, ZeroShotImageClassificationOutput, ZeroShotImageClassificationOutputElement, ZeroShotImageClassificationParameters, ZeroShotObjectDetectionInput, ZeroShotObjectDetectionInputData, ZeroShotObjectDetectionOutput, ZeroShotObjectDetectionOutputElement, index as snippets };
